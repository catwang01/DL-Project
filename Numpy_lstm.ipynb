{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# Numpy LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:55:53.286501Z",
     "start_time": "2020-07-09T15:55:53.266282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(2, 3, 4)\n",
      "(2, 4)\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "n_samples = 2\n",
    "n_sequences = 3\n",
    "n_features = 5\n",
    "x = np.random.randn(n_samples, n_sequences, n_features)\n",
    "inputs = tf.constant(x, dtype=tf.float64)\n",
    "\n",
    "lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
    "whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
    "\n",
    "print(whole_seq_output.shape)\n",
    "print(final_memory_state.shape)\n",
    "print(final_carry_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T01:16:42.614615Z",
     "start_time": "2020-07-10T01:16:42.604987Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.layers.LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:56:17.961111Z",
     "start_time": "2020-07-09T15:56:17.956625Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + exp(-x + 10e-5))\n",
    "\n",
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_x = np.exp(x-c)\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    return exp_x / (sum_exp_x + 10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T01:18:14.437017Z",
     "start_time": "2020-07-10T01:18:14.431928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T02:48:23.832949Z",
     "start_time": "2020-07-10T02:48:23.823841Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                 Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                 bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                 Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                 bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                 Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                 bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                 Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                 bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                 Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                 by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
    "    \"\"\"\n",
    "    # 初始化缓存列表\n",
    "    caches = []    \n",
    "    # 获取 x 和 参数 Wy 的维度大小\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape    \n",
    "    # 初始化 a, c 和 y 的值\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))    \n",
    "    # 初始化 a_next 和 c_next\n",
    "    a_next = a0\n",
    "    c_next = np.zeros(a_next.shape)    \n",
    "    # 循环所有时间步\n",
    "    for t in range(T_x):        \n",
    "    # 更新下一时间步隐状态值、记忆值并计算预测 \n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)        \n",
    "        # 在 a 中保存新的激活值 \n",
    "        a[:,:,t] = a_next        \n",
    "        # 在 a 中保存预测值\n",
    "        y[:,:,t] = yt        \n",
    "        # 在 c 中保存记忆值\n",
    "        c[:,:,t]  = c_next        \n",
    "        # 添加到缓存列表\n",
    "        caches.append(cache)    \n",
    "        # 保存各计算值供反向传播调用\n",
    "    caches = (caches, x)    \n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T02:48:26.725441Z",
     "start_time": "2020-07-10T02:48:26.715696Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):    \n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "    Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "    Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "    Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "    bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "    Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "    bo --  Bias of the output gate, numpy array of shape (n_a, 1)Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "    by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    c_next -- next memory state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取参数字典中各个参数\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]    \n",
    "    # 获取 xt 和 Wy 的维度参数\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape    \n",
    "    # 拼接 a_prev 和 xt\n",
    "    concat = np.zeros((n_a + n_x, m))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = xt    \n",
    "    # 计算遗忘门、更新门、记忆细胞候选值、下一时间步的记忆细胞、输出门和下一时间步的隐状态值\n",
    "    ft = sigmoid(np.matmul(Wf, concat) + bf)\n",
    "    it = sigmoid(np.matmul(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.matmul(Wc, concat) + bc)\n",
    "    c_next = ft*c_prev + it*cct\n",
    "    ot = sigmoid(np.matmul(Wo, concat) + bo)\n",
    "    a_next = ot*np.tanh(c_next)    \n",
    "    # 计算 LSTM 的预测输出\n",
    "    yt_pred = softmax(np.matmul(Wy, a_next) + by)    \n",
    "    # 保存各计算结果值\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T02:48:29.849124Z",
     "start_time": "2020-07-10T02:48:29.830895Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
    "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
    "    cache -- cache storing information from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "     dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
    "     da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "     dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)\n",
    "     dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "     dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "     dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "     dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "     dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "     dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "     dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "     dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取缓存值\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache    # 获取 xt 和 a_next 的维度大小\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_next.shape    \n",
    "    # 计算各种门的梯度\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = dc_next * it + ot * (1 - np.tanh(c_next) ** 2) * it * da_next * cct * (1 - np.tanh(cct) ** 2)\n",
    "    dit = dc_next * cct + ot * (1 - np.tanh(c_next) ** 2) * cct * da_next * it * (1 - it)\n",
    "    dft = dc_next * c_prev + ot * (1 - np.tanh(c_next) ** 2) * c_prev * da_next * ft * (1 - ft)    # 计算各参数的梯度 \n",
    "    dWf = np.dot(dft, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWi = np.dot(dit, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWc = np.dot(dcct, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWo = np.dot(dot, np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
    "    dbi = np.sum(dit, axis=1, keepdims=True)\n",
    "    dbc = np.sum(dcct, axis=1, keepdims=True)\n",
    "    dbo = np.sum(dot, axis=1, keepdims=True)\n",
    "\n",
    "    da_prev = np.dot(parameters['Wf'][:,:n_a].T, dft) + np.dot(parameters['Wi'][:,:n_a].T, dit) + np.dot(parameters['Wc'][:,:n_a].T, dcct) + np.dot(parameters['Wo'][:,:n_a].T, dot)\n",
    "    dc_prev = dc_next*ft + ot*(1-np.square(np.tanh(c_next)))*ft*da_next\n",
    "    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wi'][:,n_a:].T,dit)+np.dot(parameters['Wc'][:,n_a:].T,dcct)+np.dot(parameters['Wo'][:,n_a:].T,dot) \n",
    "\n",
    "    # 将各梯度保存至字典\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi, \n",
    "                   \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T02:48:31.306768Z",
     "start_time": "2020-07-10T02:48:31.292229Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
    "    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)\n",
    "    caches -- cache storing information from the forward pass (lstm_forward)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "           dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
    "           da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "           dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "           dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "           dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "           dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "           dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "           dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "           dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "           dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取第一个缓存值\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]    # 获取 da 和 x1 的形状大小\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape    \n",
    "    # 初始化各梯度值\n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    dc_prevt = np.zeros((n_a, m))\n",
    "    dWf = np.zeros((n_a, n_a+n_x))\n",
    "    dWi = np.zeros((n_a, n_a+n_x))\n",
    "    dWc = np.zeros((n_a, n_a+n_x))\n",
    "    dWo = np.zeros((n_a, n_a+n_x))\n",
    "    dbf = np.zeros((n_a, 1))\n",
    "    dbi = np.zeros((n_a, 1))\n",
    "    dbc = np.zeros((n_a, 1))\n",
    "    dbo = np.zeros((n_a, 1))    \n",
    "    # 循环各时间步\n",
    "    for t in reversed(range(T_x)):        \n",
    "        # 使用 lstm 单元反向传播计算各梯度值\n",
    "        gradients = lstm_cell_backward(da[:, :, t] + da_prevt, dc_prevt, caches[t])        \n",
    "        # 保存各梯度值\n",
    "        dx[:,:,t] = gradients['dxt']\n",
    "        dWf = dWf + gradients['dWf']\n",
    "        dWi = dWi + gradients['dWi']\n",
    "        dWc = dWc + gradients['dWc']\n",
    "        dWo = dWo + gradients['dWo']\n",
    "        dbf = dbf + gradients['dbf']\n",
    "        dbi = dbi + gradients['dbi']\n",
    "        dbc = dbc + gradients['dbc']\n",
    "        dbo = dbo + gradients['dbo']\n",
    "\n",
    "    da0 = gradients['da_prev']\n",
    "\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,                \n",
    "    \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T02:48:33.686450Z",
     "start_time": "2020-07-10T02:48:33.678040Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    xt shape: (n_samples, n_features)\n",
    "    a_prev shape: (n_samples, n_features_a)\n",
    "    c_prev shape: (n_samples, n_features_a)\n",
    "    \"\"\"\n",
    "    x = np.concate([xt, a_prev])\n",
    "    # Wf, bf\n",
    "    # Wi, bi\n",
    "    # Wo, bo\n",
    "    # Wc, bc\n",
    "    new_x  = np.concate([xt, a_prev])\n",
    "    Wf, bf = parameters['Wf'], parameters['bf']\n",
    "    Wi, bi = parameters['Wi'], parameters['bi']\n",
    "    Wo, bo = parameters['Wo'], parameters['bo']\n",
    "    Wc, bc = parameters['Wc'], parameters['bcc']\n",
    "    \n",
    "    ft = sigmoid(np.matmul(Wf, new_x) + bf)\n",
    "    it = sigmoid(np.matmul(Wi, new_x) + bi)\n",
    "    ot = sigmoid(np.matmul(Wo, new_x) + bo)\n",
    "    ct = np.tanh(np.matmul(Wc, new_x) + bc)\n",
    "    c_next = ft * c_prev + it * ct\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    y = softmax(a_next)\n",
    "    return a_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_forward(x)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
