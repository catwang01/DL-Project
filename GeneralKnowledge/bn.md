[toc]

# DL BN

bn 这个东西为什么有效，目前还没有一个统一的认知，笔者就搜集一些资料，将目前的一些观点进行整理罗列。

## BN 为什么要 rescale

1. 这种解释来自 [ 1 ]。认为，
    1. 如果激活函数是 sigmoid、tanh 之类的，normalization 会让数据分布集中在非饱和区，而非饱和区近似于线性，因此表达能力不足。通过rescale，将分布再放缩到饱和区以增强表达能力。
    2. 如果激活函数是 relu 的话，rescale 可以避免 dead relu 问题。

这种解释的缺点：
1. 放缩到非饱和区一会儿被当作是优点，说可以缓解 gradient vanishing。一会儿又被当作缺点，说表达能力不足。有些自相矛盾。

## BN 的作用

### 1. ics 问题

先说说什么是 ics 问题。

我们知道，输入的分布是会影响到网络的效果的。所以我们才会对输入进行标准化。而神经网络是多层神经元的叠加，我们可以将前一层看作后一层的输入，那么前一层的输入的分布也会影响到后面的层的参数的学习。

但是，在网络学习的过程中，由于参数在不断变换，每一层的输出的分布是在不断变化的。这导致输入没有一个稳定的分布，不利于参数的学习。

#### 观点1：解决了 ics 问题

这个是 bn 原 paper 中给出的解释。有人认为 BN 解决了 ics 问题，因为中间层的分布被统一放缩到了方差为0均值为1的分布。

#### 观点1反面：没有解决 ics 问题

个人认为，bn 并没有解决 ics 问题。反倒是，bn 本身就会导致 ics 问题。上面的这种解释忽略到了一个问题：bn 在标准化之后还有一个 rescale 的过程。 rescale 的参数 $\beta$   和 $\gamma$  是不断更新的，这就蕴含着每一层的分布是不断变化的。因此，bn 并没有解决 ics 问题。要解决 ics 问题，就不需要 rescale 的过程。


### 2. gradient vanishing 的问题

#### 观点2：bn 解决了 gradient vanishing 的问题

有人认为，bn 之所以会加速训练，是因为 bn 通过标准化，将数据分布拉回到了 sigmoid 激活函数的非饱和区，解决了梯度消失的问题。

#### 观点2反面：bn 没有解决 gradient vanishing 的问题

个人观点：bn 也并没有解决梯度消失问题，理由

1. 如果 bn 通过标准化将数据分布拉回到了 sigmoid 激活函数的非饱和区，那么 rescale 之后又可能返回到饱和区，何来缓解了 gradient vanishing 一说。
2. 同时，如果 bn 果真是通过将数据分布拉回到 sigmoid 激活函数的非饱和区来缓解 gradient vanishing 问题，那么 relu 函数并没有非饱和区，是不是使用了 relu 激活函数之后就不需要 bn 了呢？当然不是，resnet 中的激活函数就是 relu，但是照样还是需要 bn。
3. 无法解释将 bn 放在激活函数之后效果也不错。如果真的是为了解决 sigmoid 的非饱和问题，那么就无法解释为什么将 bn 放在激活函数之后也会有比较好的效果。

### 3. 网络解耦

#### 观点3: bn 让网络耦合

笔者比较认同观点三的解释。观点三认为，bn 通过 normalization + rescaling，将复杂的分布信息抽取成为 $\beta$ 和 $\gamma$ 两个参数。这是，下层网络在学习是，不太需要考虑上层网络之间错综复杂的关系，而主要之考虑 $\beta$ 和 $\gamma$ 两个参数就可以了。因此对于下层网路的学习更加友好，不会出现上层网络参数之间的效果相互抵消，最后梯度更新缓慢的问题。

## bn 不适合于所有的应用

这部分可以看 [ 2 ]
如在 Super-Resolution 领域，添加 BN 不如不添加。因为 SR 的 input image 和 output image 的信息很相似，如果使用 BN，就破坏了这种相似性，反而要重新学习这种相似性。导致效果不佳。

# References
1. [详解深度学习中的Normalization，BN/LN/WN - 知乎](https://zhuanlan.zhihu.com/p/33173246)
2. [(10 封私信 / 80 条消息) NTIRE2017夺冠的EDSR去掉了Batch Normalization层就获得了提高为什么？ - 知乎](https://www.zhihu.com/question/62599196)
