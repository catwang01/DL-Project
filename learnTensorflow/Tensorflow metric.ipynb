{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# Tensorflow metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | positive | negative |\n",
    "| -- | -- | -- |\n",
    "| true | TP | TN |\n",
    "| false | FP | FN |\n",
    "\n",
    "$$\n",
    "pecision = \\frac{TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个 batch 来说，可以这样计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :0.8889\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "labels = np.array([[1,1,1,0],\n",
    "                   [1,1,1,0],\n",
    "                   [1,1,1,0],\n",
    "                   [1,1,1,0]], dtype=np.uint8)\n",
    "\n",
    "predictions = np.array([[1,0,0,0],\n",
    "                        [1,1,0,0],\n",
    "                        [1,1,1,0],\n",
    "                        [0,1,1,1]], dtype=np.uint8)\n",
    "\n",
    "n_batches = len(labels)\n",
    "\n",
    "# predictions positive\n",
    "pred_p = (predictions > 0).sum()\n",
    "\n",
    "# labels == 1 && predictions == 1\n",
    "TP = (labels*predictions > 0).sum() \n",
    "\n",
    "precision = true_p / pred_p\n",
    "print(\"Precision :%1.4f\" %(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 真实：A A A C B C A B B C\n",
    "- 预测：A A C B A C A C B C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于硬件方面的一些限制，导致此方法不能扩展到大型数据集，比如当数据集很大时，就无法一次性适应内存。 因而，为了使其可扩展，我们希望使评估指标能够逐步更新，每批新的预测和标签。 为此，我们需要跟踪两个值。\n",
    "\n",
    "*   TP 正确预测的正样本数量\n",
    "*   TP + FP，即预测样本中所有正样本的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 所以我们要这么做"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们修改为下面的版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize running variables\n",
    "TP = 0\n",
    "PRED_P = 0\n",
    "\n",
    "# Specific steps\n",
    "# Create running variables\n",
    "TP = 0\n",
    "PRED_P = 0\n",
    "\n",
    "def reset_running_variables():\n",
    "    \"\"\" Resets the previous values of running variables to zero \"\"\"\n",
    "    global TP, PRED_P\n",
    "    TP = 0\n",
    "    PRED_P = 0\n",
    "\n",
    "def update_running_variables(labels, preds):\n",
    "    global TP, PRED_P\n",
    "    TP += ((labels * preds) > 0).sum()\n",
    "    PRED_P += (preds > 0).sum()\n",
    "\n",
    "def calculate_precision():\n",
    "    global TP, PRED_P\n",
    "    return float (TP) / PRED_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 怎么用上面的函数呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的两个例子，给出了运用的具体代码，并且可以更好滴帮助我们理解`tf.metrics.precision()`的计算逻辑以及对应输出所代表的含义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 样本整体准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NP] SCORE: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# Overall precision\n",
    "reset_running_variables()\n",
    "\n",
    "for i in range(n_batches):\n",
    "    update_running_variables(labels=labels[i], preds=predictions[i])\n",
    "\n",
    "precision = calculate_precision()\n",
    "print(\"[NP] SCORE: %1.4f\" %precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批次准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NP] batch 0 score: 1.0000\n",
      "[NP] batch 1 score: 1.0000\n",
      "[NP] batch 2 score: 1.0000\n",
      "[NP] batch 3 score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "# Batch precision\n",
    "\n",
    "for i in range(n_batches):\n",
    "    reset_running_variables() # 每一个 batch 重置一下\n",
    "    update_running_variables(labels=labels[i], preds=predictions[i])\n",
    "    prec = calculate_precision()\n",
    "    print(\"[NP] batch %d score: %1.4f\" %(i, prec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Github代码中precision的解释部分\n",
    "\n",
    "> The `precision` function creates **two local variables**, `true_positives` and `false_positives`, that are used to compute the precision. This value is ultimately returned as `precision`, an idempotent operation that simply divides `true_positives` by the sum of `true_positives` and `false_positives`. For estimation of the metric over a stream of data, the function creates an `update_op` operation that updates these variables and returns the `precision`.\n",
    "\n",
    "- 两个变量和 `tf.metrics.precision()`的关系\n",
    "\n",
    "官方文档提及的**two local variables** ：`true_postives` 和 `false_positives`分别对应上文定义的两个变量。 *true_postives--N_TRUE_P* false_postives--N_PRED_P - N_TRUE_P\n",
    "\n",
    "### 三个函数和头大的`update_op`\n",
    "\n",
    "官方文档提及的`update_op`和`precision`分别对应上文定义的两个函数：\n",
    "- precision : calculate_precision()\n",
    "- update_op: update_running_variables()\n",
    "\n",
    "大家不要被这个`update_op`搞晕，其实从字面来理解就是一个变量更新的操作，上文的代码中，就是通过`reset_running_variables()`的位置来决定何时对变量进行更新，其实就是对应于`tf.variables_initializer()`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overall 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] SCORE: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# Overall precision using tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# 在 graph 下定义节点\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Placeholders to take in batches onf data\n",
    "    tf_label = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    tf_prediction = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "\n",
    "    # Define the metric and update operations\n",
    "    tf_metric, tf_metric_update = tf.metrics.precision(tf_label,\n",
    "                                                      tf_prediction,\n",
    "                                                      name=\"my_metric\")\n",
    "\n",
    "    # Isolate the variables stored behind the scenes by the metric operation\n",
    "    running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"my_metric\")\n",
    "\n",
    "    # Define initializer to initialize/reset running variables\n",
    "    running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # initialize/reset the running variables\n",
    "    session.run(running_vars_initializer)\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        # Update the running variables on new batch of samples\n",
    "        feed_dict = {tf_label: labels[i], tf_prediction: predictions[i]}\n",
    "        session.run(tf_metric_update, feed_dict=feed_dict)\n",
    "\n",
    "    # Calculate the score\n",
    "    score = session.run(tf_metric)\n",
    "    print(\"[TF] SCORE: %1.4f\" %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] batch 0 score: 1.0000\n",
      "[TF] batch 1 score: 1.0000\n",
      "[TF] batch 2 score: 1.0000\n",
      "[TF] batch 3 score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 在 graph 下定义节点\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Placeholders to take in batches onf data\n",
    "    tf_label = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    tf_prediction = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "\n",
    "    # Define the metric and update operations\n",
    "    tf_metric, tf_metric_update = tf.metrics.precision(tf_label,\n",
    "                                                      tf_prediction,\n",
    "                                                      name=\"my_metric\")\n",
    "\n",
    "    # Isolate the variables stored behind the scenes by the metric operation\n",
    "    running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"my_metric\")\n",
    "\n",
    "    # Define initializer to initialize/reset running variables\n",
    "    running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        # initialize/reset the running variables\n",
    "        session.run(running_vars_initializer)\n",
    "        # Update the running variables on new batch of samples\n",
    "        feed_dict = {tf_label: labels[i], tf_prediction: predictions[i]}\n",
    "        session.run(tf_metric_update, feed_dict=feed_dict)\n",
    "\n",
    "        # Calculate the score\n",
    "        score = session.run(tf_metric)\n",
    "        print(\"[TF] batch %d score: %1.4f\" %(i, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.8333333)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "predictions = np.array([0, 0, 2, 1, 0, 2, 0, 2, 1, 2])\n",
    "targets = np.array([0, 0, 0, 2, 1, 2, 0, 1, 1, 2])\n",
    "\n",
    "placeholder_predictions = tf.placeholder(tf.int32, [None])\n",
    "placeholder_targets = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "recall = tf.metrics.recall(labels=placeholder_targets, predictions=placeholder_predictions)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    ret = sess.run(recall, feed_dict={placeholder_predictions: predictions, \n",
    "                                      placeholder_targets: targets})\n",
    "    print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "predictions = np.array([0, 0, 2, 1, 0, 2, 0, 2, 1, 2])\n",
    "targets = np.array([0, 0, 0, 2, 1, 2, 0, 1, 1, 2])\n",
    "\n",
    "micro_recall = recall_score(predictions, targets, average='micro')\n",
    "macro_recall = recall_score(predictions, targets, average='macro')\n",
    "print(micro_recall)\n",
    "print(macro_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value recall/false_negatives/count\n\t [[node recall/false_negatives/count/read (defined at <ipython-input-100-48d88e54ae9d>:16) ]]\n\nOriginal stack trace for 'recall/false_negatives/count/read':\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3147, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-100-48d88e54ae9d>\", line 33, in <module>\n    sess.run(metric(placeholder_targets, placeholder_predictions),\n  File \"<ipython-input-100-48d88e54ae9d>\", line 16, in metric\n    predictions=tf.equal(y_pred, k)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py\", line 2196, in recall\n    name=None)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py\", line 1562, in false_negatives\n    updates_collections)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py\", line 1495, in _count_condition\n    count = metric_variable([], dtypes.float32, name='count')\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py\", line 86, in metric_variable\n    name=name)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 1755, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 86, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4253, in identity\n    \"Identity\", input=input, name=name)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value recall/false_negatives/count\n\t [[{{node recall/false_negatives/count/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-48d88e54ae9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     sess.run(metric(placeholder_targets, placeholder_predictions), \n\u001b[1;32m     34\u001b[0m              feed_dict={placeholder_predictions: predictions, \n\u001b[0;32m---> 35\u001b[0;31m                         placeholder_targets: targets})\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value recall/false_negatives/count\n\t [[node recall/false_negatives/count/read (defined at <ipython-input-100-48d88e54ae9d>:16) ]]\n\nOriginal stack trace for 'recall/false_negatives/count/read':\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3147, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-100-48d88e54ae9d>\", line 33, in <module>\n    sess.run(metric(placeholder_targets, placeholder_predictions),\n  File \"<ipython-input-100-48d88e54ae9d>\", line 16, in metric\n    predictions=tf.equal(y_pred, k)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py\", line 2196, in recall\n    name=None)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py\", line 1562, in false_negatives\n    updates_collections)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py\", line 1495, in _count_condition\n    count = metric_variable([], dtypes.float32, name='count')\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py\", line 86, in metric_variable\n    name=name)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 1755, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 86, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4253, in identity\n    \"Identity\", input=input, name=name)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/tars/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "placeholder_predictions = tf.placeholder(tf.int32, [None])\n",
    "placeholder_targets = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "num_labels = 3\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    recall_n = [0] * num_labels\n",
    "    precision_n = [0] * num_labels\n",
    "    update_op_rec_n = [[]] * num_labels\n",
    "    update_op_pre_n = [[]] * num_labels\n",
    "    for k in range(num_labels):\n",
    "        recall_n[k], update_op_rec_n[k] = tf.metrics.recall(\n",
    "            labels=tf.equal(y_true, k),\n",
    "            predictions=tf.equal(y_pred, k)\n",
    "        )    \n",
    "        precision_n[k], update_op_pre_n[k] = tf.metrics.precision(\n",
    "            labels=tf.equal(y_true, k),\n",
    "            predictions=tf.equal(y_pred, k)\n",
    "        )\n",
    "    recall_value = sum(recall_n) * 1.0 / num_labels\n",
    "    precision_value = sum(precision_n) * 1.0 / num_labels\n",
    "    update_op_rec = sum(update_op_rec_n) * 1.0 / num_labels\n",
    "    update_op_pre = sum(update_op_pre_n) * 1.0 / num_labels\n",
    "    recall = (recall_value, update_op_rec)\n",
    "    precision = (precision_value, update_op_pre)\n",
    "    return recall, precision\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(metric(placeholder_targets, placeholder_predictions), \n",
    "             feed_dict={placeholder_predictions: predictions, \n",
    "                        placeholder_targets: targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(per_example_loss, label_ids, logits, num_labels):\n",
    "    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "    accuracy = tf.metrics.accuracy(label_ids, predictions)\n",
    "    y_true = label_ids\n",
    "    y_pred = tf.argmax(logits, 1)\n",
    "    recall_n = [0] * num_labels\n",
    "    precision_n = [0] * num_labels\n",
    "    update_op_rec_n = [[]] * num_labels\n",
    "    update_op_pre_n = [[]] * num_labels\n",
    "    for k in range(num_labels):\n",
    "        recall_n[k], update_op_rec_n[k] = tf.metrics.recall(\n",
    "            labels=tf.equal(y_true, k),\n",
    "            predictions=tf.equal(y_pred, k)\n",
    "        )    \n",
    "        precision_n[k], update_op_pre_n[k] = tf.metrics.precision(\n",
    "            labels=tf.equal(y_true, k),\n",
    "            predictions=tf.equal(y_pred, k)\n",
    "        )    \n",
    "    recall_value = sum(recall_n) * 1.0 / num_labels\n",
    "    precision_value = sum(precision_n) * 1.0 / num_labels\n",
    "    update_op_rec = sum(update_op_rec_n) * 1.0 / num_labels\n",
    "    update_op_pre = sum(update_op_pre_n) * 1.0 / num_labels\n",
    "    recall = (recall_value, update_op_rec)\n",
    "    precision = (precision_value, update_op_pre)\n",
    "    loss = tf.metrics.mean(per_example_loss)\n",
    "    return {\n",
    "        \"eval_accuracy\": accuracy,\n",
    "        \"eval_loss\": loss,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [TensorFlow: “Attempting to use uninitialized value” in variable initialization - Stack Overflow](https://stackoverflow.com/questions/44624648/tensorflow-attempting-to-use-uninitialized-value-in-variable-initialization/44630421)\n",
    "\n",
    "2. [Tensorflow使用tf.metrics计算多分类效果指标 - 知乎](https://zhuanlan.zhihu.com/p/82183796)\n",
    "\n",
    "3. [【0.2】Tensorflow踩坑记之tf.metrics - 知乎](https://zhuanlan.zhihu.com/p/43359894)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multiclass\"\"\"\n",
    "\n",
    "__author__ = \"Guillaume Genthial\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.metrics_impl import _streaming_confusion_matrix\n",
    "\n",
    "\n",
    "def precision(labels, predictions, num_classes, pos_indices=None,\n",
    "              weights=None, average='micro'):\n",
    "    \"\"\"Multi-class precision metric for Tensorflow\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : Tensor of tf.int32 or tf.int64\n",
    "        The true labels\n",
    "    predictions : Tensor of tf.int32 or tf.int64\n",
    "        The predictions, same shape as labels\n",
    "    num_classes : int\n",
    "        The number of classes\n",
    "    pos_indices : list of int, optional\n",
    "        The indices of the positive classes, default is all\n",
    "    weights : Tensor of tf.int32, optional\n",
    "        Mask, must be of compatible shape with labels\n",
    "    average : str, optional\n",
    "        'micro': counts the total number of true positives, false\n",
    "            positives, and false negatives for the classes in\n",
    "            `pos_indices` and infer the metric from it.\n",
    "        'macro': will compute the metric separately for each class in\n",
    "            `pos_indices` and average. Will not account for class\n",
    "            imbalance.\n",
    "        'weighted': will compute the metric separately for each class in\n",
    "            `pos_indices` and perform a weighted average by the total\n",
    "            number of true labels for each class.\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (scalar float Tensor, update_op)\n",
    "    \"\"\"\n",
    "    cm, op = _streaming_confusion_matrix(\n",
    "        labels, predictions, num_classes, weights)\n",
    "    pr, _, _ = metrics_from_confusion_matrix(\n",
    "        cm, pos_indices, average=average)\n",
    "    op, _, _ = metrics_from_confusion_matrix(\n",
    "        op, pos_indices, average=average)\n",
    "    return (pr, op)\n",
    "\n",
    "\n",
    "def recall(labels, predictions, num_classes, pos_indices=None, weights=None,\n",
    "           average='micro'):\n",
    "    \"\"\"Multi-class recall metric for Tensorflow\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : Tensor of tf.int32 or tf.int64\n",
    "        The true labels\n",
    "    predictions : Tensor of tf.int32 or tf.int64\n",
    "        The predictions, same shape as labels\n",
    "    num_classes : int\n",
    "        The number of classes\n",
    "    pos_indices : list of int, optional\n",
    "        The indices of the positive classes, default is all\n",
    "    weights : Tensor of tf.int32, optional\n",
    "        Mask, must be of compatible shape with labels\n",
    "    average : str, optional\n",
    "        'micro': counts the total number of true positives, false\n",
    "            positives, and false negatives for the classes in\n",
    "            `pos_indices` and infer the metric from it.\n",
    "        'macro': will compute the metric separately for each class in\n",
    "            `pos_indices` and average. Will not account for class\n",
    "            imbalance.\n",
    "        'weighted': will compute the metric separately for each class in\n",
    "            `pos_indices` and perform a weighted average by the total\n",
    "            number of true labels for each class.\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (scalar float Tensor, update_op)\n",
    "    \"\"\"\n",
    "    cm, op = _streaming_confusion_matrix(\n",
    "        labels, predictions, num_classes, weights)\n",
    "    _, re, _ = metrics_from_confusion_matrix(\n",
    "        cm, pos_indices, average=average)\n",
    "    _, op, _ = metrics_from_confusion_matrix(\n",
    "        op, pos_indices, average=average)\n",
    "    return (re, op)\n",
    "\n",
    "\n",
    "def f1(labels, predictions, num_classes, pos_indices=None, weights=None,\n",
    "       average='micro'):\n",
    "    return fbeta(labels, predictions, num_classes, pos_indices, weights,\n",
    "                 average)\n",
    "\n",
    "\n",
    "def fbeta(labels, predictions, num_classes, pos_indices=None, weights=None,\n",
    "          average='micro', beta=1):\n",
    "    \"\"\"Multi-class fbeta metric for Tensorflow\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : Tensor of tf.int32 or tf.int64\n",
    "        The true labels\n",
    "    predictions : Tensor of tf.int32 or tf.int64\n",
    "        The predictions, same shape as labels\n",
    "    num_classes : int\n",
    "        The number of classes\n",
    "    pos_indices : list of int, optional\n",
    "        The indices of the positive classes, default is all\n",
    "    weights : Tensor of tf.int32, optional\n",
    "        Mask, must be of compatible shape with labels\n",
    "    average : str, optional\n",
    "        'micro': counts the total number of true positives, false\n",
    "            positives, and false negatives for the classes in\n",
    "            `pos_indices` and infer the metric from it.\n",
    "        'macro': will compute the metric separately for each class in\n",
    "            `pos_indices` and average. Will not account for class\n",
    "            imbalance.\n",
    "        'weighted': will compute the metric separately for each class in\n",
    "            `pos_indices` and perform a weighted average by the total\n",
    "            number of true labels for each class.\n",
    "    beta : int, optional\n",
    "        Weight of precision in harmonic mean\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (scalar float Tensor, update_op)\n",
    "    \"\"\"\n",
    "    cm, op = _streaming_confusion_matrix(\n",
    "        labels, predictions, num_classes, weights)\n",
    "    _, _, fbeta = metrics_from_confusion_matrix(\n",
    "        cm, pos_indices, average=average, beta=beta)\n",
    "    _, _, op = metrics_from_confusion_matrix(\n",
    "        op, pos_indices, average=average, beta=beta)\n",
    "    return (fbeta, op)\n",
    "\n",
    "\n",
    "def safe_div(numerator, denominator):\n",
    "    \"\"\"Safe division, return 0 if denominator is 0\"\"\"\n",
    "    numerator, denominator = tf.to_float(numerator), tf.to_float(denominator)\n",
    "    zeros = tf.zeros_like(numerator, dtype=numerator.dtype)\n",
    "    denominator_is_zero = tf.equal(denominator, zeros)\n",
    "    return tf.where(denominator_is_zero, zeros, numerator / denominator)\n",
    "\n",
    "\n",
    "def pr_re_fbeta(cm, pos_indices, beta=1):\n",
    "    \"\"\"Uses a confusion matrix to compute precision, recall and fbeta\"\"\"\n",
    "    num_classes = cm.shape[0]\n",
    "    neg_indices = [i for i in range(num_classes) if i not in pos_indices]\n",
    "    cm_mask = np.ones([num_classes, num_classes])\n",
    "    cm_mask[neg_indices, neg_indices] = 0\n",
    "    diag_sum = tf.reduce_sum(tf.diag_part(cm * cm_mask))\n",
    "\n",
    "    cm_mask = np.ones([num_classes, num_classes])\n",
    "    cm_mask[:, neg_indices] = 0\n",
    "    tot_pred = tf.reduce_sum(cm * cm_mask)\n",
    "\n",
    "    cm_mask = np.ones([num_classes, num_classes])\n",
    "    cm_mask[neg_indices, :] = 0\n",
    "    tot_gold = tf.reduce_sum(cm * cm_mask)\n",
    "\n",
    "    pr = safe_div(diag_sum, tot_pred)\n",
    "    re = safe_div(diag_sum, tot_gold)\n",
    "    fbeta = safe_div((1. + beta**2) * pr * re, beta**2 * pr + re)\n",
    "\n",
    "    return pr, re, fbeta\n",
    "\n",
    "\n",
    "def metrics_from_confusion_matrix(cm, pos_indices=None, average='micro',\n",
    "                                  beta=1):\n",
    "    \"\"\"Precision, Recall and F1 from the confusion matrix\n",
    "    Parameters\n",
    "    ----------\n",
    "    cm : tf.Tensor of type tf.int32, of shape (num_classes, num_classes)\n",
    "        The streaming confusion matrix.\n",
    "    pos_indices : list of int, optional\n",
    "        The indices of the positive classes\n",
    "    beta : int, optional\n",
    "        Weight of precision in harmonic mean\n",
    "    average : str, optional\n",
    "        'micro', 'macro' or 'weighted'\n",
    "    \"\"\"\n",
    "    num_classes = cm.shape[0]\n",
    "    if pos_indices is None:\n",
    "        pos_indices = [i for i in range(num_classes)]\n",
    "\n",
    "    if average == 'micro':\n",
    "        return pr_re_fbeta(cm, pos_indices, beta)\n",
    "    elif average in {'macro', 'weighted'}:\n",
    "        precisions, recalls, fbetas, n_golds = [], [], [], []\n",
    "        for idx in pos_indices:\n",
    "            pr, re, fbeta = pr_re_fbeta(cm, [idx], beta)\n",
    "            precisions.append(pr)\n",
    "            recalls.append(re)\n",
    "            fbetas.append(fbeta)\n",
    "            cm_mask = np.zeros([num_classes, num_classes])\n",
    "            cm_mask[idx, :] = 1\n",
    "            n_golds.append(tf.to_float(tf.reduce_sum(cm * cm_mask)))\n",
    "\n",
    "        if average == 'macro':\n",
    "            pr = tf.reduce_mean(precisions)\n",
    "            re = tf.reduce_mean(recalls)\n",
    "            fbeta = tf.reduce_mean(fbetas)\n",
    "            return pr, re, fbeta\n",
    "        if average == 'weighted':\n",
    "            n_gold = tf.reduce_sum(n_golds)\n",
    "            pr_sum = sum(p * n for p, n in zip(precisions, n_golds))\n",
    "            pr = safe_div(pr_sum, n_gold)\n",
    "            re_sum = sum(r * n for r, n in zip(recalls, n_golds))\n",
    "            re = safe_div(re_sum, n_gold)\n",
    "            fbeta_sum = sum(f * n for f, n in zip(fbetas, n_golds))\n",
    "            fbeta = safe_div(fbeta_sum, n_gold)\n",
    "            return pr, re, fbeta\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5833333\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = [0, 0, 2, 1, 0, 2, 0, 2, 1, 2]\n",
    "y_pred = [0, 0, 0, 2, 1, 2, 0, 1, 1, 2]\n",
    "\n",
    "pos_indices = None  # Class 0 is the 'negative' class\n",
    "num_classes = 3\n",
    "average = 'macro'\n",
    "\n",
    "# Tuple of (value, update_op)\n",
    "macro_precision = precision(y_true, y_pred, num_classes, pos_indices, average='macro')\n",
    "micro_precision = precision(y_true, y_pred, num_classes, pos_indices, average='micro')\n",
    "# recall = recall(y_true, y_pred, num_classes, pos_indices, average=average)\n",
    "# f2 = fbeta(y_true, y_pred, num_classes, pos_indices, average=average, beta=2)\n",
    "# f1 =f1(y_true, y_pred, num_classes, pos_indices, average=average)\n",
    "\n",
    "# Run the update op and get the updated value\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    print(sess.run(macro_precision[1]))\n",
    "    print(sess.run(micro_precision[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tars]",
   "language": "python",
   "name": "conda-env-tars-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
