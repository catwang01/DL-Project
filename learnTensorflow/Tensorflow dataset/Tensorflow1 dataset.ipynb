{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tensorflow1 dataset\n",
    "tags: 小书匠,tensorflow,tensorflow1,dataset,make_csv_dataset\n",
    "grammar_cjkRuby: true\n",
    "# renderNumberedHeading: true\n",
    "---\n",
    "\n",
    "[toc!]\n",
    "\n",
    "# Tensorflow1 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow1 和 tensorflow2 的 dataset 的用法略有不同，这里是 tensorflow1 中的 dataset 的用法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道，在TensorFlow中可以使用feed-dict的方式输入数据信息，但是这种方法的速度是最慢的，在实际应用中应该尽量避免这种方法。而使用输入管道就可以保证GPU在工作时无需等待新的数据输入，这才是正确的方法。\n",
    "\n",
    "幸运的是，TensorFlow提供了一种内置的API——Dataset，使得我们可以很容易地就利用输入管道的方式输入数据。在这篇教程中，我们将介绍如何创建和使用输入管道以及如何高效地向模型输入数据。\n",
    "\n",
    "## 概述\n",
    "\n",
    "使用Dataset的三个步骤：\n",
    "\n",
    "1. 载入数据：为数据创建一个Dataset实例\n",
    "\n",
    "2. 创建一个迭代器：使用创建的数据集来构造一个Iterator实例以遍历数据集\n",
    "\n",
    "3. 使用数据：使用创建的迭代器，我们可以从数据集中获取数据元素，从而输入到模型中去。\n",
    "\n",
    "许多 tensorflow 的高层 api 如 tf.keras，tf.estimator 中，2、3 步骤被封装起来，我们只需要完成第 1 步就好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据\n",
    "\n",
    "首先，我们需要将一些数据放到数据集中。\n",
    "\n",
    "### 从numpy载入\n",
    "\n",
    "这是最常见的情况，假设我们有一个numpy数组，我们想将它传递给TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:42:20.933787Z",
     "start_time": "2021-03-11T03:42:20.733034Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a random vector of shape (100,2)\n",
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以传递多个numpy数组，最典型的例子是当数据被划分为特征和标签的时候："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:42:37.572940Z",
     "start_time": "2021-03-11T03:42:37.563913Z"
    }
   },
   "outputs": [],
   "source": [
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从tensors中载入\n",
    "\n",
    "我们当然也可以用一些张量初始化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:43:05.031709Z",
     "start_time": "2021-03-11T03:43:04.943264Z"
    }
   },
   "outputs": [],
   "source": [
    "# using a tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从placeholder中载入\n",
    "\n",
    "如果我们想动态地改变Dataset中的数据，使用这种方式是很有用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:43:13.753005Z",
     "start_time": "2021-03-11T03:43:13.738608Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从generator载入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以从generator中初始化一个Dataset。当一个数组中元素长度不相同时，使用这种方式处理是很有效的。（例如一个序列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:55:55.802047Z",
     "start_time": "2021-03-11T03:55:55.775646Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence = np.array([[1],[2,3],[3,4]])\n",
    "\n",
    "def generator():\n",
    "    for el in sequence:\n",
    "        yield el\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator,\n",
    "                                           output_types=tf.float32,\n",
    "                                           output_shapes=tf.TensorShape([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这种情况下，你还需要指定数据的类型和大小以创建正确的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T15:16:23.201922Z",
     "start_time": "2021-03-10T15:16:23.065117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: /Users/ed/.keras/datasets/./data\n",
      "test_path: /Users/ed/.keras/datasets/./data\n",
      "(array([6.4, 2.8, 5.6, 2.2], dtype=float32), 2)\n",
      "(array([5. , 2.3, 3.3, 1. ], dtype=float32), 1)\n",
      "(array([4.9, 2.5, 4.5, 1.7], dtype=float32), 2)\n",
      "(array([4.9, 3.1, 1.5, 0.1], dtype=float32), 0)\n",
      "(array([5.7, 3.8, 1.7, 0.3], dtype=float32), 0)\n",
      "(array([4.4, 3.2, 1.3, 0.2], dtype=float32), 0)\n",
      "(array([5.4, 3.4, 1.5, 0.4], dtype=float32), 0)\n",
      "(array([6.9, 3.1, 5.1, 2.3], dtype=float32), 2)\n",
      "(array([6.7, 3.1, 4.4, 1.4], dtype=float32), 1)\n",
      "(array([5.1, 3.7, 1.5, 0.4], dtype=float32), 0)\n",
      "(array([5.2, 2.7, 3.9, 1.4], dtype=float32), 1)\n",
      "(array([6.9, 3.1, 4.9, 1.5], dtype=float32), 1)\n",
      "(array([5.8, 4. , 1.2, 0.2], dtype=float32), 0)\n",
      "(array([5.4, 3.9, 1.7, 0.4], dtype=float32), 0)\n",
      "(array([7.7, 3.8, 6.7, 2.2], dtype=float32), 2)\n",
      "(array([6.3, 3.3, 4.7, 1.6], dtype=float32), 1)\n",
      "(array([6.8, 3.2, 5.9, 2.3], dtype=float32), 2)\n",
      "(array([7.6, 3. , 6.6, 2.1], dtype=float32), 2)\n",
      "(array([6.4, 3.2, 5.3, 2.3], dtype=float32), 2)\n",
      "(array([5.7, 4.4, 1.5, 0.4], dtype=float32), 0)\n",
      "(array([6.7, 3.3, 5.7, 2.1], dtype=float32), 2)\n",
      "(array([6.4, 2.8, 5.6, 2.1], dtype=float32), 2)\n",
      "(array([5.4, 3.9, 1.3, 0.4], dtype=float32), 0)\n",
      "(array([6.1, 2.6, 5.6, 1.4], dtype=float32), 2)\n",
      "(array([7.2, 3. , 5.8, 1.6], dtype=float32), 2)\n",
      "(array([5.2, 3.5, 1.5, 0.2], dtype=float32), 0)\n",
      "(array([5.8, 2.6, 4. , 1.2], dtype=float32), 1)\n",
      "(array([5.9, 3. , 5.1, 1.8], dtype=float32), 2)\n",
      "(array([5.4, 3. , 4.5, 1.5], dtype=float32), 1)\n",
      "(array([6.7, 3. , 5. , 1.7], dtype=float32), 1)\n",
      "(array([6.3, 2.3, 4.4, 1.3], dtype=float32), 1)\n",
      "(array([5.1, 2.5, 3. , 1.1], dtype=float32), 1)\n",
      "(array([6.4, 3.2, 4.5, 1.5], dtype=float32), 1)\n",
      "(array([6.8, 3. , 5.5, 2.1], dtype=float32), 2)\n",
      "(array([6.2, 2.8, 4.8, 1.8], dtype=float32), 2)\n",
      "(array([6.9, 3.2, 5.7, 2.3], dtype=float32), 2)\n",
      "(array([6.5, 3.2, 5.1, 2. ], dtype=float32), 2)\n",
      "(array([5.8, 2.8, 5.1, 2.4], dtype=float32), 2)\n",
      "(array([5.1, 3.8, 1.5, 0.3], dtype=float32), 0)\n",
      "(array([4.8, 3. , 1.4, 0.3], dtype=float32), 0)\n",
      "(array([7.9, 3.8, 6.4, 2. ], dtype=float32), 2)\n",
      "(array([5.8, 2.7, 5.1, 1.9], dtype=float32), 2)\n",
      "(array([6.7, 3. , 5.2, 2.3], dtype=float32), 2)\n",
      "(array([5.1, 3.8, 1.9, 0.4], dtype=float32), 0)\n",
      "(array([4.7, 3.2, 1.6, 0.2], dtype=float32), 0)\n",
      "(array([6. , 2.2, 5. , 1.5], dtype=float32), 2)\n",
      "(array([4.8, 3.4, 1.6, 0.2], dtype=float32), 0)\n",
      "(array([7.7, 2.6, 6.9, 2.3], dtype=float32), 2)\n",
      "(array([4.6, 3.6, 1. , 0.2], dtype=float32), 0)\n",
      "(array([7.2, 3.2, 6. , 1.8], dtype=float32), 2)\n",
      "(array([5. , 3.3, 1.4, 0.2], dtype=float32), 0)\n",
      "(array([6.6, 3. , 4.4, 1.4], dtype=float32), 1)\n",
      "(array([6.1, 2.8, 4. , 1.3], dtype=float32), 1)\n",
      "(array([5. , 3.2, 1.2, 0.2], dtype=float32), 0)\n",
      "(array([7. , 3.2, 4.7, 1.4], dtype=float32), 1)\n",
      "(array([6. , 3. , 4.8, 1.8], dtype=float32), 2)\n",
      "(array([7.4, 2.8, 6.1, 1.9], dtype=float32), 2)\n",
      "(array([5.8, 2.7, 5.1, 1.9], dtype=float32), 2)\n",
      "(array([6.2, 3.4, 5.4, 2.3], dtype=float32), 2)\n",
      "(array([5. , 2. , 3.5, 1. ], dtype=float32), 1)\n",
      "(array([5.6, 2.5, 3.9, 1.1], dtype=float32), 1)\n",
      "(array([6.7, 3.1, 5.6, 2.4], dtype=float32), 2)\n",
      "(array([6.3, 2.5, 5. , 1.9], dtype=float32), 2)\n",
      "(array([6.4, 3.1, 5.5, 1.8], dtype=float32), 2)\n",
      "(array([6.2, 2.2, 4.5, 1.5], dtype=float32), 1)\n",
      "(array([7.3, 2.9, 6.3, 1.8], dtype=float32), 2)\n",
      "(array([4.4, 3. , 1.3, 0.2], dtype=float32), 0)\n",
      "(array([7.2, 3.6, 6.1, 2.5], dtype=float32), 2)\n",
      "(array([6.5, 3. , 5.5, 1.8], dtype=float32), 2)\n",
      "(array([5. , 3.4, 1.5, 0.2], dtype=float32), 0)\n",
      "(array([4.7, 3.2, 1.3, 0.2], dtype=float32), 0)\n",
      "(array([6.6, 2.9, 4.6, 1.3], dtype=float32), 1)\n",
      "(array([5.5, 3.5, 1.3, 0.2], dtype=float32), 0)\n",
      "(array([7.7, 3. , 6.1, 2.3], dtype=float32), 2)\n",
      "(array([6.1, 3. , 4.9, 1.8], dtype=float32), 2)\n",
      "(array([4.9, 3.1, 1.5, 0.1], dtype=float32), 0)\n",
      "(array([5.5, 2.4, 3.8, 1.1], dtype=float32), 1)\n",
      "(array([5.7, 2.9, 4.2, 1.3], dtype=float32), 1)\n",
      "(array([6. , 2.9, 4.5, 1.5], dtype=float32), 1)\n",
      "(array([6.4, 2.7, 5.3, 1.9], dtype=float32), 2)\n",
      "(array([5.4, 3.7, 1.5, 0.2], dtype=float32), 0)\n",
      "(array([6.1, 2.9, 4.7, 1.4], dtype=float32), 1)\n",
      "(array([6.5, 2.8, 4.6, 1.5], dtype=float32), 1)\n",
      "(array([5.6, 2.7, 4.2, 1.3], dtype=float32), 1)\n",
      "(array([6.3, 3.4, 5.6, 2.4], dtype=float32), 2)\n",
      "(array([4.9, 3.1, 1.5, 0.1], dtype=float32), 0)\n",
      "(array([6.8, 2.8, 4.8, 1.4], dtype=float32), 1)\n",
      "(array([5.7, 2.8, 4.5, 1.3], dtype=float32), 1)\n",
      "(array([6. , 2.7, 5.1, 1.6], dtype=float32), 1)\n",
      "(array([5. , 3.5, 1.3, 0.3], dtype=float32), 0)\n",
      "(array([6.5, 3. , 5.2, 2. ], dtype=float32), 2)\n",
      "(array([6.1, 2.8, 4.7, 1.2], dtype=float32), 1)\n",
      "(array([5.1, 3.5, 1.4, 0.3], dtype=float32), 0)\n",
      "(array([4.6, 3.1, 1.5, 0.2], dtype=float32), 0)\n",
      "(array([6.5, 3. , 5.8, 2.2], dtype=float32), 2)\n",
      "(array([4.6, 3.4, 1.4, 0.3], dtype=float32), 0)\n",
      "(array([4.6, 3.2, 1.4, 0.2], dtype=float32), 0)\n",
      "(array([7.7, 2.8, 6.7, 2. ], dtype=float32), 2)\n",
      "(array([5.9, 3.2, 4.8, 1.8], dtype=float32), 1)\n",
      "(array([5.1, 3.8, 1.6, 0.2], dtype=float32), 0)\n",
      "(array([4.9, 3. , 1.4, 0.2], dtype=float32), 0)\n",
      "(array([4.9, 2.4, 3.3, 1. ], dtype=float32), 1)\n",
      "(array([4.5, 2.3, 1.3, 0.3], dtype=float32), 0)\n",
      "(array([5.8, 2.7, 4.1, 1. ], dtype=float32), 1)\n",
      "(array([5. , 3.4, 1.6, 0.4], dtype=float32), 0)\n",
      "(array([5.2, 3.4, 1.4, 0.2], dtype=float32), 0)\n",
      "(array([5.3, 3.7, 1.5, 0.2], dtype=float32), 0)\n",
      "(array([5. , 3.6, 1.4, 0.2], dtype=float32), 0)\n",
      "(array([5.6, 2.9, 3.6, 1.3], dtype=float32), 1)\n",
      "(array([4.8, 3.1, 1.6, 0.2], dtype=float32), 0)\n",
      "(array([6.3, 2.7, 4.9, 1.8], dtype=float32), 2)\n",
      "(array([5.7, 2.8, 4.1, 1.3], dtype=float32), 1)\n",
      "(array([5. , 3. , 1.6, 0.2], dtype=float32), 0)\n",
      "(array([6.3, 3.3, 6. , 2.5], dtype=float32), 2)\n",
      "(array([5. , 3.5, 1.6, 0.6], dtype=float32), 0)\n",
      "(array([5.5, 2.6, 4.4, 1.2], dtype=float32), 1)\n",
      "(array([5.7, 3. , 4.2, 1.2], dtype=float32), 1)\n",
      "(array([4.4, 2.9, 1.4, 0.2], dtype=float32), 0)\n",
      "(array([4.8, 3. , 1.4, 0.1], dtype=float32), 0)\n",
      "(array([5.5, 2.4, 3.7, 1. ], dtype=float32), 1)\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import csv\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "def downloadfiles():\n",
    "    train_path = tf.keras.utils.get_file(fname=r'./data', origin=TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(fname=r'./data', origin=TEST_URL)\n",
    "    return train_path, test_path\n",
    "\n",
    "train_path,test_path = downloadfiles()\n",
    "print(\"train_path: {}\\ntest_path: {}\".format(train_path, test_path))\n",
    "\n",
    "# 注意，这两个不能用 list, 而得是 tuple，否则会报错 unhashable type: 'list'\n",
    "output_types = (tf.float32, tf.int32)\n",
    "output_shapes = (tf.TensorShape([None]), tf.TensorShape([]))\n",
    "\n",
    "def data_generator(file_path):\n",
    "    with open(file_path) as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        for row in reader:\n",
    "            label = row.pop()\n",
    "            features = row\n",
    "            yield features, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(data_generator, train_path), \n",
    "        output_types=output_types,\n",
    "        output_shapes=output_shapes\n",
    ")\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_batch))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从 csv 文件中创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic_file:  /Users/ed/.keras/datasets/train.csv\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py:499: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py:212: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From <ipython-input-1-9ea1c67c33aa>:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "(OrderedDict([('sex', array([b'male', b'male', b'male', b'male'], dtype=object)), ('age', array([32., 21., 30., 24.], dtype=float32)), ('n_siblings_spouses', array([0, 0, 0, 2], dtype=int32)), ('parch', array([0, 0, 0, 0], dtype=int32)), ('fare', array([ 7.925,  8.05 , 27.75 , 73.5  ], dtype=float32)), ('class', array([b'Third', b'Third', b'First', b'Second'], dtype=object)), ('deck', array([b'unknown', b'unknown', b'C', b'unknown'], dtype=object)), ('embark_town', array([b'Southampton', b'Southampton', b'Cherbourg', b'Southampton'],\n",
      "      dtype=object)), ('alone', array([b'y', b'y', b'y', b'n'], dtype=object))]), array([0, 0, 0, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import csv\n",
    "\n",
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "print(\"titanic_file: \", titanic_file)\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(titanic_file, \n",
    "                                                batch_size=4, \n",
    "                                                label_name=\"survived\")\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.data.experimental.make_csv_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建一个迭代器\n",
    "\n",
    "我们已经知道了如何创建数据集，但是如何从中获取数据呢？我们需要使用一个Iterator遍历数据集并重新得到数据真实值。有四种形式的迭代器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot Iterator\n",
    "\n",
    "这是最简单的迭代器，下面给出第一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:59:51.448048Z",
     "start_time": "2021-03-11T03:59:51.367812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76747849 0.04158681]\n",
      "[0.8919353  0.63975712]\n",
      "[0.74729546 0.70326269]\n",
      "[0.66525039 0.51324041]\n",
      "[0.37313983 0.3916572 ]\n",
      "[0.30910254 0.59993772]\n",
      "[0.59367427 0.22685718]\n",
      "[0.41151761 0.08332528]\n",
      "[0.45145689 0.73832339]\n",
      "[0.12102829 0.01189038]\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "x = np.random.sample((10, 2))\n",
    "\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "# create the iterator\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# 调用get_next()来获得包含数据的张量\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            ret = sess.run(next_batch)\n",
    "            print(ret)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可初始化的迭代器\n",
    "\n",
    "如果我们想建立一个可以在运行时改变数据源的动态数据集，我们可以用placeholder 创建一个数据集。接着用常见的feed-dict机制初始化这个placeholder。这些工作可以通过使用一个可初始化的迭代器完成。使用上一节的第三个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T04:01:14.842511Z",
     "start_time": "2021-03-11T04:01:14.754546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9798317 0.1434286]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = np.random.sample((100,2))\n",
    "# using a placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "iterator = dataset.make_initializable_iterator() # create the iteratorator\n",
    "next_batch = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    # feed the placeholder with data\n",
    "    sess.run(iterator.initializer, feed_dict={ x: data })\n",
    "    print(sess.run(next_batch)) # output [ 0.52374458  0.71968478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializable iterator to switch between dataset\n",
    "EPOCHS = 10\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "#     switch to test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 可重新初始化的迭代器\n",
    "\n",
    "这个概念和之前的相似，我们想在数据间动态切换。但是我们是转换数据集而不是把新数据送到相同的数据集。和之前一样，我们需要一个训练集和一个测试集\n",
    "\n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "\n",
    "接下来创建两个Dataset\n",
    "\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "\n",
    "现在我们要用到一个小技巧，即创建一个通用的Iterator\n",
    "\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "\n",
    "接着创建两个初始化运算\n",
    "\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "和之前一样，我们得到下一个元素\n",
    "\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "现在，我们可以直接使用session运行两个初始化运算。把上面这些综合起来我们可以得到：\n",
    "\n",
    "# Reinitializable iterator to switch between Datasets\n",
    "EPOCHS = 10\n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op) # switch to train dataset\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "    sess.run(test_init_op) # switch to val dataset\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用数据\n",
    "\n",
    "在之前的例子中，我们使用session来打印Dataset中next元素的值\n",
    "\n",
    "```\n",
    "...\n",
    "next_el = iter.get_next()\n",
    "...\n",
    "print(sess.run(next_el)) # will output the current element\n",
    "```\n",
    "\n",
    "现在为了向模型传递数据，我们只需要传递get_next()产生的张量。\n",
    "\n",
    "在下面的代码中，我们有一个包含两个numpy数组的Dataset，这里用到了和第一节一样的例子。注意到我们需要将.random.sample封装到另外一个numpy数组中，因此会增加一个维度以用于数据batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:50:09.859821Z",
     "start_time": "2021-03-11T03:50:09.122284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-1396308222e8>:11: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1474d2780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1474d2780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1474d2780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1474d2780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x14742df60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x14742df60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x14742df60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x14742df60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x147522278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x147522278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x147522278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x147522278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Iter: 0, Loss: 0.4063\n",
      "Iter: 1, Loss: 0.3880\n",
      "Iter: 2, Loss: 0.3702\n",
      "Iter: 3, Loss: 0.3531\n",
      "Iter: 4, Loss: 0.3366\n",
      "Iter: 5, Loss: 0.3208\n",
      "Iter: 6, Loss: 0.3056\n",
      "Iter: 7, Loss: 0.2911\n",
      "Iter: 8, Loss: 0.2773\n",
      "Iter: 9, Loss: 0.2641\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]),\n",
    "                    np.array([np.random.sample((100,1))]))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "x, y = iterator.get_next()\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有用的技巧\n",
    "\n",
    "### batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常情况下，batch是一件麻烦的事情，但是通过Dataset API我们可以使用batch(BATCH_SIZE)方法自动地将数据按照指定的大小batch，默认值是1。在接下来的例子中，我们使用的batch大小为4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:51:03.585420Z",
     "start_time": "2021-03-11T03:51:03.523855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.73756691e-01 4.69362655e-01]\n",
      " [9.67855623e-01 3.44047169e-01]\n",
      " [5.17740357e-01 2.13477521e-01]\n",
      " [7.90970741e-04 9.16710425e-01]]\n"
     ]
    }
   ],
   "source": [
    "# BATCHING\n",
    "BATCH_SIZE = 4\n",
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "el = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [2 3 0 0]]\n",
      "[[4 5 5 0]\n",
      " [7 8 0 0]]\n",
      "[[9 0 0 0]\n",
      " [0 1 0 0]]\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = [[1, 0, 0], [2, 3, 0], [4, 5, 5], [7, 8, 0], [9, 0, 0], [0, 1, 0]]\n",
    "\n",
    "#tf.TensorShape([])     表示长度为单个数字\n",
    "#tf.TensorShape([None]) 表示不 padding\n",
    "padded_shapes = tf.TensorShape([4]) # 指将一个 instance padding 成形状为 [4] 的\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.padded_batch(2, padded_shapes=padded_shapes)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_batch))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat\n",
    "\n",
    "使用.repeat()我们可以指定数据集迭代的次数。如果没有设置参数，则迭代会一直循环。通常来说，一直循环并直接用标准循环控制epoch的次数能取得较好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle\n",
    "\n",
    "我们可以使用shuffle()方法将Dataset随机洗牌，默认是在数据集中对每一个epoch洗牌，这种处理可以避免过拟合。\n",
    "\n",
    "我们也可以设置buffer_size参数，下一个元素将从这个固定大小的缓存中按照均匀分布抽取。例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "\n",
    "你可以使用map()方法对数据集的每个成员应用自定义的函数。在下面的例子中，我们将每个元素乘以2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:57:32.278860Z",
     "start_time": "2021-03-11T03:57:32.208305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[4]\n",
      "[6]\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "# MAP\n",
    "\n",
    "def func(x): # func 接受的是一个 Tensor\n",
    "    return x * 2\n",
    "\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.map(func)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(len(x)):\n",
    "        print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，func 的参数是一个 Tensor 类型，因此，我们只能在这里进行一些 Tensor 类型才支持的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://localhost:8888/lab/workspaces/auto-i/tree/DL-Project/learnTensorflow/Tensorflow%20dataset/Tensorflow1%20dataset.ipynb\n",
    "2. [TensorFlow学习笔记(4): Tensorflow tf.data.Dataset - 知乎](https://zhuanlan.zhihu.com/p/37106443)\n",
    "3. [tf.data: Build TensorFlow input pipelines  |  TensorFlow Core](https://www.tensorflow.org/guide/data#consuming_csv_da2a3)\n",
    "4. [tf.data.experimental.make_csv_dataset  |  TensorFlow Core v2.4.1](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
