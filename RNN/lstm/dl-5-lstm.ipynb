{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# DL-学习笔记-5-自定义LSTM层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先明确输入数据。 LSTM 接受的是序列数据。按照 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "batch_size = 2\n",
    "n_sequences = 3\n",
    "n_features = 5\n",
    "units = 4\n",
    "x_train = tf.random.normal(shape=(batch_size, n_sequences, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customLSTM1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customLSTM1 是我们的一第种实现方式。直接使用上面的公式就可以。\n",
    "\n",
    "在 tensorflow 的源码中，LSTM 的功能是由 LSTMCell 和 LSTM 两个类实现和。这个进行了简化。将它们合并成一个类。其中  lst_cell_forward 参数的功能对应于源码中 LSTMCell 的 call 方法的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLSTM1(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, name=\"customLSTM1\", **kwargs):\n",
    "        super(customLSTM1, self).__init__(name=name, **kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.Wf = self.add_weight(\"Wf\", shape=(self.units + input_dim, self.units))\n",
    "        self.bf = self.add_weight(\"bf\", shape=(1, self.units))\n",
    "        self.Wi = self.add_weight(\"Wi\", shape=(self.units + input_dim, self.units))\n",
    "        self.bi = self.add_weight(\"bi\", shape=(1, self.units))\n",
    "        self.Wo = self.add_weight(\"Wo\", shape=(self.units + input_dim, self.units))\n",
    "        self.bo = self.add_weight(\"bo\", shape=(1, self.units))\n",
    "        self.Wc = self.add_weight(\"Wc\", shape=(self.units + input_dim, self.units))\n",
    "        self.bc = self.add_weight(\"bc\", shape=(1, self.units))\n",
    "        super(customLSTM1, self).build(input_shape)\n",
    "\n",
    "    def lstm_cell_forward(self, xt, c_prev, h_prev):\n",
    "        concat_x = tf.concat([h_prev, xt], axis=1)\n",
    "        forget_gate = tf.nn.sigmoid(tf.matmul(concat_x, self.Wf) + self.bf)\n",
    "        input_gate = tf.nn.sigmoid(tf.matmul(concat_x, self.Wi) + self.bi)\n",
    "        output_gate = tf.nn.sigmoid(tf.matmul(concat_x, self.Wo) + self.bo)\n",
    "        ctt = tf.nn.tanh(tf.matmul(concat_x, self.Wc) + self.bc)\n",
    "        c_next = forget_gate * c_prev + input_gate * ctt\n",
    "        h_next = output_gate * tf.nn.tanh(c_next)\n",
    "        return h_next, c_next, h_next\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, n_sequences = x.shape[0], x.shape[1]\n",
    "        c_prev = tf.zeros(shape=(batch_size, self.units))\n",
    "        h_prev = tf.zeros(shape=(batch_size, self.units))\n",
    "        for t in range(n_sequences):\n",
    "            h_next, c_next, _ = self.lstm_cell_forward(x[:, t, :], c_prev, h_prev)\n",
    "            h_prev = h_prev\n",
    "            c_prev = c_next\n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据，可以正确输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[ 0.3015624 ,  0.5244602 , -0.20986232, -0.00497146],\n",
       "       [ 0.33407864,  0.51461583, -0.02144831,  0.15913488]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylstm1 = customLSTM1(units)\n",
    "mylstm1.build(input_shape=(None, n_sequences, n_features))\n",
    "mylstm1(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customLSTM2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customLSTM2 是 lstm 的另一种实现。和 customLSTM2 的主要区别是将 Wr, Wi, Wo, Wc 归结为两个参数，kernel 和 recurrent_kernel。\n",
    "这和 RNN 正好是对应起来的。\n",
    "\n",
    "并且将 bf、bi、bo、bc 四个 bias 也拼接起来形成一个。这样做矩阵乘法时相当于 4 个一起来做，可以提高效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后的代码我们会将 customLSTM2 的结果和 tf.keras.layers.LSTM 的结果做对比。为了两个代码可以得到相同的结果，我们需要定义一个函数来自\n",
    "初始化权重。\n",
    "\n",
    "这个函数接受 shape 和 dtype 两个参数， 返回一个 tf.Variable 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(shape, dtype):\n",
    "    tf.random.set_seed(123)\n",
    "    return tf.Variable(tf.random.normal(shape=shape, dtype=dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们实现 `customLSTM2` 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLSTM2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units,\n",
    "                 name=\"customLSTM2\",\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                 recurrent_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                 bias_initlizer=tf.keras.initializers.zeros(),\n",
    "                 activation=tf.nn.tanh,\n",
    "                 recurrent_activation=tf.nn.sigmoid,\n",
    "                 unit_forget_bias=True, **kwargs):\n",
    "\n",
    "        super(customLSTM2, self).__init__(name=name, **kwargs)\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.recurrent_initializer = recurrent_initializer\n",
    "        self.bias_initializer = bias_initlizer\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(name=\"kernel\", shape=(input_dim, self.units * 4), initializer=self.kernel_initializer)\n",
    "        self.recurrent_kernel = self.add_weight(name=\"recurrent_kernel\", shape=(self.units, self.units * 4), initializer=self.recurrent_initializer)\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "                def bias_initializer(_, dtype):\n",
    "                    return tf.concat([\n",
    "                        self.bias_initializer(shape=(1, self.units), dtype=dtype),\n",
    "                        tf.ones(shape=(1, self.units), dtype=dtype), # forget_gate  的 bias\n",
    "                        self.bias_initializer(shape=(1, self.units * 2), dtype=dtype)\n",
    "                    ], axis=1)\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            self.bias = self.add_weight(name=\"bias\", shape=(1, self.units * 4), initializer=bias_initializer)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        super(customLSTM2, self).build(input_shape)\n",
    "\n",
    "    def lstm_cell_forward(self, xt, c_prev, h_prev):\n",
    "        z = tf.matmul(xt, self.kernel)\n",
    "        z += tf.matmul(h_prev, self.recurrent_kernel)\n",
    "        if self.use_bias:\n",
    "            z += self.bias\n",
    "        z0, z1, z2, z3 = tf.split(z, 4, axis=-1)\n",
    "        input_gate = self.recurrent_activation(z0)\n",
    "        forget_gate = self.recurrent_activation(z1)\n",
    "        ctt = self.activation(z2)\n",
    "        output_gate = self.recurrent_activation(z3)\n",
    "        c_next = forget_gate * c_prev + input_gate * ctt\n",
    "        h_next = output_gate * self.activation(c_next)\n",
    "        return h_next, c_next, h_next\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, n_sequences = x.shape[0], x.shape[1]\n",
    "        c_prev = tf.zeros(shape=(batch_size, self.units))\n",
    "        h_prev = tf.zeros(shape=(batch_size, self.units))\n",
    "        for t in range(n_sequences):\n",
    "            h_next, c_next, _ = self.lstm_cell_forward(x[:, t, :], c_prev, h_prev)\n",
    "            h_prev = h_next\n",
    "            c_prev = c_next\n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的 `customLSTM2` 的类除了使用 kernel 和 recurrent_kernel 以及 bias 三个参数来代替原来的 8 个参数之外，没有其它本质变化。\n",
    "\n",
    "不过，`customLSTM2` 参考了很大程度上参考了 `tf.keras.layers.LSTM`，参数名和变量名也尽可能保持一致，可以看做是 `tf.keras.layers.LSTM` 的阉割版。\n",
    "\n",
    "其中 unit_forget_bias 参数需要特殊说明，unit_forget_bias = True 时，forget_gate 的 bias 会被初始化为 1，否则会被初始化为0。tensorflow 默认为 True。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和 tensorflow 自带的 tf.keras.layers.LSTM 做比较。结果完全相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylstm2 = customLSTM2(units, kernel_initializer=get_weight, recurrent_initializer=get_weight)\n",
    "mylstm2.build(input_shape=(None, n_sequences, n_features))\n",
    "\n",
    "lstm = tf.keras.layers.LSTM(units, kernel_initializer=get_weight, recurrent_initializer=get_weight)\n",
    "mylstm2(x_train) == lstm(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，customLSTM1 和 customLSTM2 在 tf.keras.layers.LSTM 类中都实现了 。tf.keras.layers.LSTM 有一个 implement 参数，就是指定实现方式。 impletement = 1 表明实现是 customLSTM1 的实现方式，否则是 customLSTM2 的实现方式。\n",
    "\n",
    "两种实现方式没有什么本质区别。只不过 cusotmLSTM2 将四个权重放在一起计算，因此需要更多的内存，但是可能有提升效率。而 customLSTM1 会比较节省内存。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
