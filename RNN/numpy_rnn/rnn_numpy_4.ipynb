{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# RNN Numpy 实现四 完整版"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:35:07.499092Z",
     "start_time": "2020-07-02T10:35:07.494536Z"
    }
   },
   "source": [
    "基于前面的一些代码，我们来将训练的代码完整化，进行梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:35:16.517913Z",
     "start_time": "2020-07-02T10:35:16.174270Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "input_word = list(\"I am learning RNN\")\n",
    "words = set(input_word)\n",
    "input_word_onehot = OneHotEncoder(sparse=False).fit_transform(np.array(input_word).reshape(-1, 1))\n",
    "\n",
    "# 序列长度取 3\n",
    "sequenceLen = 3\n",
    "\n",
    "x, y = [], []\n",
    "for i in range(len(input_word_onehot) - sequenceLen):\n",
    "    x.append(input_word_onehot[i:i + sequenceLen])\n",
    "    y.append(input_word_onehot[i + 1: i + 1 + sequenceLen])\n",
    "\n",
    "x_train = np.array(x)\n",
    "y_train = np.array(y)\n",
    "\n",
    "n_class = len(words)\n",
    "nx = n_class\n",
    "ny = n_class\n",
    "nh = 4\n",
    "\n",
    "def get_weights(shape, dtype=np.float32):\n",
    "    np.random.seed(123)\n",
    "    return np.array(np.random.randn(*shape), dtype=dtype)\n",
    "\n",
    "def get_bias(shape, dtype=np.float32):\n",
    "    return np.zeros(shape, dtype=dtype)\n",
    "\n",
    "# 权重初始化，为了之后操作方便，放在一个字典中\n",
    "weights = {\n",
    "    'Wxh': get_weights((nx, nh)),\n",
    "    'Why': get_weights((nh, ny)),\n",
    "    'Whh': get_weights((nh, nh)),\n",
    "    'bh': get_bias((1, nh)),\n",
    "    'by': get_bias((1, ny))\n",
    "}\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp\n",
    "    return y\n",
    "\n",
    "def xentropy(y, yhat):\n",
    "    return np.mean(np.sum(-y * np.log(yhat + 1e-8), axis=0))\n",
    "\n",
    "def _forward(xs, weights):\n",
    "    Why = weights['Why']\n",
    "    Whh = weights['Whh']\n",
    "    Wxh = weights['Wxh']\n",
    "    bh = weights['bh']\n",
    "    by = weights['by']\n",
    "\n",
    "    n_sequence = xs.shape[0]\n",
    "    ny = Why.shape[1]\n",
    "    nh = Wxh.shape[1]\n",
    "\n",
    "    a = np.zeros((n_sequence, nh))\n",
    "    h = np.zeros((n_sequence, nh))\n",
    "    o = np.zeros((n_sequence, ny))\n",
    "    yhat = np.zeros((n_sequence, ny))\n",
    "    hprev = None\n",
    "\n",
    "    for t, x in enumerate(xs):\n",
    "        if t == 0:\n",
    "            hprev = np.zeros((1, nh))\n",
    "        else:\n",
    "            hprev = h[t - 1]\n",
    "\n",
    "        a[t] = np.matmul(x, Wxh) + np.matmul(hprev, Whh) + bh\n",
    "        h[t] = np.tanh(a[t])\n",
    "        o[t] = np.matmul(h[t], Why) + by\n",
    "        yhat[t] = softmax(o[t])\n",
    "    return yhat, a, h, o\n",
    "\n",
    "def _backward(xs, ys, weights, a, o, h, yhat):\n",
    "    n_sequences = xs.shape[0]\n",
    "\n",
    "    Why = weights['Why']\n",
    "    Whh = weights['Whh']\n",
    "    Wxh = weights['Wxh']\n",
    "    bh = weights['bh']\n",
    "    by = weights['by']\n",
    "\n",
    "    grads = { name: np.zeros_like(weights[name]) for name in weights}\n",
    "    danext = None\n",
    "    for i in range(n_sequences - 1, -1, -1):\n",
    "        if i == n_sequences - 1:\n",
    "            danext = np.zeros_like(a[i:i + 1])\n",
    "\n",
    "        dot = yhat[i:i + 1] - ys[i:i + 1]\n",
    "\n",
    "        # backprop through ot\n",
    "        dby = dot\n",
    "        dWhy = np.matmul(h[i:i + 1].T, dot)\n",
    "        dht = np.matmul(dot, Why.T) + np.matmul(danext, Whh.T)\n",
    "        dWhh = np.matmul(h[i:i + 1].T, danext)\n",
    "\n",
    "        # backprop through ht\n",
    "        dat = dht * (1 - h[i:i + 1] ** 2)\n",
    "\n",
    "        # backprop through at\n",
    "        dWxh = np.matmul(xs[i:i + 1].T, dat)\n",
    "        dbh = dat\n",
    "\n",
    "        # 累加梯度\n",
    "        grads['by'] += dby\n",
    "        grads['bh'] += dbh\n",
    "        grads['Whh'] += dWhh\n",
    "        grads['Wxh'] += dWxh\n",
    "        grads['Why'] += dWhy\n",
    "        danext = dat\n",
    "\n",
    "    for k in grads:\n",
    "        grads[k] = grads[k] / n_sequences\n",
    "    return grads\n",
    "\n",
    "def forward(batch_xs, weights):\n",
    "    batch_yhat = []\n",
    "    batch_a = []\n",
    "    batch_o = []\n",
    "    batch_h = []\n",
    "\n",
    "    for xs in batch_xs:\n",
    "        yhat, a, h, o = _forward(xs, weights)\n",
    "        batch_yhat.append(yhat)\n",
    "        batch_o.append(o)\n",
    "        batch_h.append(h)\n",
    "        batch_a.append(a)\n",
    "\n",
    "    batch_yhat = np.array(batch_yhat)\n",
    "    batch_a = np.array(batch_a)\n",
    "    batch_h = np.array(batch_h)\n",
    "    batch_o = np.array(batch_o)\n",
    "    return batch_yhat, batch_a, batch_h, batch_o\n",
    "\n",
    "def backward(batch_xs, batch_ys, weights, batch_a, batch_o, batch_h, batch_yhat):\n",
    "    n_batch = batch_xs.shape[0]\n",
    "    grads = { name: np.zeros_like(weights[name]) for name in weights}\n",
    "    for xs, ys, a, o, h, yhat in zip(batch_xs, batch_ys, batch_a, batch_o, batch_h, batch_yhat):\n",
    "        tmp_grads = _backward(xs, ys, weights, a, o, h, yhat)\n",
    "        for k in tmp_grads:\n",
    "            grads[k] += tmp_grads[k]\n",
    "    for k in grads:\n",
    "        grads[k] /= n_batch\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个 sgd，来更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:35:16.522069Z",
     "start_time": "2020-07-02T10:35:16.519578Z"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(grads, weights, lr=0.1):\n",
    "    for name in weights:\n",
    "        weights[name] -= lr * grads[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:34:25.926503Z",
     "start_time": "2020-07-02T10:34:25.842217Z"
    }
   },
   "source": [
    "训练模型，可以看到 loss 不断减少！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:35:16.905855Z",
     "start_time": "2020-07-02T10:35:16.523766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Train Loss: 4.222423528244957\n",
      "Epoch: 1/100 Train Loss: 3.7168807073561934\n",
      "Epoch: 2/100 Train Loss: 3.4110927045930355\n",
      "Epoch: 3/100 Train Loss: 3.0785505858347264\n",
      "Epoch: 4/100 Train Loss: 2.7974501645702885\n",
      "Epoch: 5/100 Train Loss: 2.576097749103156\n",
      "Epoch: 6/100 Train Loss: 2.387566511186096\n",
      "Epoch: 7/100 Train Loss: 2.2219042945493475\n",
      "Epoch: 8/100 Train Loss: 2.0911166642398875\n",
      "Epoch: 9/100 Train Loss: 2.00142374218148\n",
      "Epoch: 10/100 Train Loss: 1.9323612189859039\n",
      "Epoch: 11/100 Train Loss: 1.8738544014147673\n",
      "Epoch: 12/100 Train Loss: 1.8219660172287577\n",
      "Epoch: 13/100 Train Loss: 1.7745208783750472\n",
      "Epoch: 14/100 Train Loss: 1.729935647161311\n",
      "Epoch: 15/100 Train Loss: 1.6869280762731453\n",
      "Epoch: 16/100 Train Loss: 1.644712598260227\n",
      "Epoch: 17/100 Train Loss: 1.6035307565963801\n",
      "Epoch: 18/100 Train Loss: 1.5645356932416574\n",
      "Epoch: 19/100 Train Loss: 1.5284995263637502\n",
      "Epoch: 20/100 Train Loss: 1.4952450159396196\n",
      "Epoch: 21/100 Train Loss: 1.4642122600945904\n",
      "Epoch: 22/100 Train Loss: 1.4348952339111616\n",
      "Epoch: 23/100 Train Loss: 1.4069300751247669\n",
      "Epoch: 24/100 Train Loss: 1.3800681020478562\n",
      "Epoch: 25/100 Train Loss: 1.3541383184654825\n",
      "Epoch: 26/100 Train Loss: 1.3290185443036893\n",
      "Epoch: 27/100 Train Loss: 1.3046144314504853\n",
      "Epoch: 28/100 Train Loss: 1.2808440511176429\n",
      "Epoch: 29/100 Train Loss: 1.2576270733574386\n",
      "Epoch: 30/100 Train Loss: 1.234878022312644\n",
      "Epoch: 31/100 Train Loss: 1.2125037436923196\n",
      "Epoch: 32/100 Train Loss: 1.1904058466222724\n",
      "Epoch: 33/100 Train Loss: 1.1684911229988242\n",
      "Epoch: 34/100 Train Loss: 1.1466964061179696\n",
      "Epoch: 35/100 Train Loss: 1.1250355845788171\n",
      "Epoch: 36/100 Train Loss: 1.1036586795759762\n",
      "Epoch: 37/100 Train Loss: 1.0828577606500356\n",
      "Epoch: 38/100 Train Loss: 1.0629363065443738\n",
      "Epoch: 39/100 Train Loss: 1.04403577117944\n",
      "Epoch: 40/100 Train Loss: 1.0261240252869368\n",
      "Epoch: 41/100 Train Loss: 1.0090992071867484\n",
      "Epoch: 42/100 Train Loss: 0.992857185036743\n",
      "Epoch: 43/100 Train Loss: 0.9773075406569541\n",
      "Epoch: 44/100 Train Loss: 0.9623728801354139\n",
      "Epoch: 45/100 Train Loss: 0.9479864219894476\n",
      "Epoch: 46/100 Train Loss: 0.9340902247656779\n",
      "Epoch: 47/100 Train Loss: 0.9206336961637549\n",
      "Epoch: 48/100 Train Loss: 0.9075724423921672\n",
      "Epoch: 49/100 Train Loss: 0.8948672970805167\n",
      "Epoch: 50/100 Train Loss: 0.8824836755737812\n",
      "Epoch: 51/100 Train Loss: 0.8703911060494708\n",
      "Epoch: 52/100 Train Loss: 0.8585630955853724\n",
      "Epoch: 53/100 Train Loss: 0.8469771755187819\n",
      "Epoch: 54/100 Train Loss: 0.8356151654174298\n",
      "Epoch: 55/100 Train Loss: 0.8244634916843776\n",
      "Epoch: 56/100 Train Loss: 0.8135133516494829\n",
      "Epoch: 57/100 Train Loss: 0.8027606613396185\n",
      "Epoch: 58/100 Train Loss: 0.7922054852093313\n",
      "Epoch: 59/100 Train Loss: 0.7818509370919551\n",
      "Epoch: 60/100 Train Loss: 0.7717017631305381\n",
      "Epoch: 61/100 Train Loss: 0.761762807800997\n",
      "Epoch: 62/100 Train Loss: 0.7520378647066386\n",
      "Epoch: 63/100 Train Loss: 0.7425290344978803\n",
      "Epoch: 64/100 Train Loss: 0.7332367543615874\n",
      "Epoch: 65/100 Train Loss: 0.7241603385744144\n",
      "Epoch: 66/100 Train Loss: 0.7152986465897617\n",
      "Epoch: 67/100 Train Loss: 0.7066507655306481\n",
      "Epoch: 68/100 Train Loss: 0.6982163841512827\n",
      "Epoch: 69/100 Train Loss: 0.6899957726626753\n",
      "Epoch: 70/100 Train Loss: 0.6819894806374879\n",
      "Epoch: 71/100 Train Loss: 0.6741977207318466\n",
      "Epoch: 72/100 Train Loss: 0.6666196839639951\n",
      "Epoch: 73/100 Train Loss: 0.6592530808479966\n",
      "Epoch: 74/100 Train Loss: 0.6520937617495279\n",
      "Epoch: 75/100 Train Loss: 0.6451357546243837\n",
      "Epoch: 76/100 Train Loss: 0.6383714725923271\n",
      "Epoch: 77/100 Train Loss: 0.6317921741579919\n",
      "Epoch: 78/100 Train Loss: 0.6253883847701676\n",
      "Epoch: 79/100 Train Loss: 0.619150356682841\n",
      "Epoch: 80/100 Train Loss: 0.613068431370464\n",
      "Epoch: 81/100 Train Loss: 0.6071332857604895\n",
      "Epoch: 82/100 Train Loss: 0.6013360903474202\n",
      "Epoch: 83/100 Train Loss: 0.5956686128144201\n",
      "Epoch: 84/100 Train Loss: 0.5901231914636612\n",
      "Epoch: 85/100 Train Loss: 0.5846927263413164\n",
      "Epoch: 86/100 Train Loss: 0.5793706497921464\n",
      "Epoch: 87/100 Train Loss: 0.5741508494445914\n",
      "Epoch: 88/100 Train Loss: 0.5690275852802321\n",
      "Epoch: 89/100 Train Loss: 0.5639954660046494\n",
      "Epoch: 90/100 Train Loss: 0.5590493378012221\n",
      "Epoch: 91/100 Train Loss: 0.5541842513256872\n",
      "Epoch: 92/100 Train Loss: 0.5493953978736037\n",
      "Epoch: 93/100 Train Loss: 0.544678058579722\n",
      "Epoch: 94/100 Train Loss: 0.5400275350005055\n",
      "Epoch: 95/100 Train Loss: 0.535439094604929\n",
      "Epoch: 96/100 Train Loss: 0.5309079334232137\n",
      "Epoch: 97/100 Train Loss: 0.5264290838577497\n",
      "Epoch: 98/100 Train Loss: 0.5219973878264927\n",
      "Epoch: 99/100 Train Loss: 0.5176074272261605\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, a, h, o = forward(x_train, weights)\n",
    "    loss = xentropy(y_train, yhat)\n",
    "    grads = backward(x_train, y_train, weights, a, o, h, yhat)\n",
    "    sgd(grads, weights, lr=1)\n",
    "    print(f\"Epoch: {epoch}/{n_epochs} Train Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
