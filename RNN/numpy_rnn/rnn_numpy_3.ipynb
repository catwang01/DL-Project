{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# RNN Numpy 实现三 batch版本 —— 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 版本代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:07:36.512762Z",
     "start_time": "2020-07-10T03:07:36.507821Z"
    }
   },
   "source": [
    "基于上一节的代码，我们可以很简单得转化为 batch 版本的代码。 我们直接给出代码，读者可以和上一小节的代码做对比。\n",
    "\n",
    "其中的 `_forward` 和  `_backward` 函数实际上就是上一节的 `forward` 和 `backward` 函数。只是改了个名字而已。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:11.704299Z",
     "start_time": "2020-07-10T03:14:11.365368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wxh\n",
      "[[-6.23370055e-03  1.04331255e-01  2.23508645e-02  1.82005409e-02]\n",
      " [-1.54104326e-02 -3.32658598e-03 -9.04711414e-05 -1.50557747e-02]\n",
      " [ 8.19845311e-03  3.85644659e-02 -3.31628546e-02 -4.46483115e-04]\n",
      " [ 1.11874761e-02  1.59848616e-01 -2.25059371e-02  4.80860807e-02]\n",
      " [ 4.41634879e-02  7.01074908e-03  4.23029438e-02  9.37758684e-02]\n",
      " [ 8.51382967e-03 -4.64937501e-02 -1.44467270e-02  1.96995456e-02]\n",
      " [-2.08229781e-03 -1.86435338e-02 -1.15537411e-02  5.49922101e-02]\n",
      " [-1.82566326e-02  1.89795326e-02 -3.24461833e-02 -1.32172499e-02]\n",
      " [-1.33043816e-02 -1.09042577e-01  2.38811933e-02  1.03872553e-01]\n",
      " [ 8.72361809e-02 -3.63293923e-02 -1.23410830e-02 -1.97669547e-02]\n",
      " [ 1.14516485e-02 -4.60840343e-03  3.22320871e-02  2.18383409e-02]\n",
      " [ 6.77897185e-02  1.38952746e-03 -5.20812087e-02 -5.00065386e-02]]\n",
      "Why\n",
      "[[-0.01225391  0.0329933  -0.02681418 -0.00649567 -0.05570216  0.1297125\n",
      "  -0.03257607  0.01844829  0.10973296 -0.0778837  -0.01666309 -0.06249831]\n",
      " [ 0.0232008  -0.03062716 -0.07174666  0.00251815  0.00192849  0.10913263\n",
      "   0.11045782  0.05964187 -0.03315546 -0.04200283 -0.0741348  -0.05521285]\n",
      " [-0.01224116  0.00566421  0.11142957 -0.06263226  0.04732599 -0.00419463\n",
      "  -0.03436515 -0.00857714 -0.01389913 -0.00123701 -0.00276442 -0.02450886]\n",
      " [ 0.05996267  0.01078336 -0.05233568  0.03773454 -0.07731857 -0.10633167\n",
      "  -0.09503508  0.00923052  0.10124347 -0.0650825   0.14232817  0.03482077]]\n",
      "Whh\n",
      "[[ 0.15446615 -0.00124231 -0.05664341 -0.16452336]\n",
      " [ 0.13315888  0.08278523 -0.04694551 -0.12312789]\n",
      " [ 0.0983765   0.00854117  0.00313957  0.06394426]\n",
      " [ 0.04713476 -0.08556058 -0.04893308 -0.06553293]]\n",
      "bh\n",
      "[[ 0.18325336  0.11167993 -0.05786112  0.26197213]]\n",
      "by\n",
      "[[-0.09955249  0.05676479  0.04074105 -0.0183601  -0.03368903  0.07818042\n",
      "   0.04349705  0.01167354  0.00267598 -0.02386261 -0.01919586 -0.03887272]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "input_word = list(\"I am learning RNN\")\n",
    "words = set(input_word)\n",
    "input_word_onehot = OneHotEncoder(sparse=False).fit_transform(np.array(input_word).reshape(-1, 1))\n",
    "\n",
    "# 序列长度取 3\n",
    "sequenceLen = 3\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(input_word_onehot) - sequenceLen):\n",
    "    x.append(input_word_onehot[i:i + sequenceLen])\n",
    "    y.append(input_word_onehot[i + 1: i + 1 + sequenceLen])\n",
    "\n",
    "x_train = np.array(x)\n",
    "y_train = np.array(y)\n",
    "\n",
    "\n",
    "def get_weights(shape, dtype=np.float32):\n",
    "    np.random.seed(123)\n",
    "    return np.array(np.random.randn(*shape), dtype=dtype)\n",
    "\n",
    "def get_bias(shape, dtype=np.float32):\n",
    "    return np.zeros(shape, dtype=dtype)\n",
    "\n",
    "\n",
    "n_features = len(words)\n",
    "nx = n_features\n",
    "ny = n_features\n",
    "nh = 4\n",
    "\n",
    "weights = {\n",
    "    'Wxh': get_weights((nx, nh)),\n",
    "    'Why': get_weights((nh, ny)),\n",
    "    'Whh': get_weights((nh, nh)),\n",
    "    'bh': get_bias((1, nh)),\n",
    "    'by': get_bias((1, ny))\n",
    "}\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp\n",
    "    return y\n",
    "\n",
    "def _forward(xs, weights):\n",
    "    \"\"\"\n",
    "    xs: shape=(n_sequences, n_features)\n",
    "    \"\"\"\n",
    "    Why = weights['Why']\n",
    "    Whh = weights['Whh']\n",
    "    Wxh = weights['Wxh']\n",
    "    bh = weights['bh']\n",
    "    by = weights['by']\n",
    "\n",
    "    n_sequence = xs.shape[0]\n",
    "    ny = Why.shape[1]\n",
    "    nh = Wxh.shape[1]\n",
    "\n",
    "    a = np.zeros((n_sequence, nh))\n",
    "    h = np.zeros((n_sequence, nh))\n",
    "    o = np.zeros((n_sequence, ny))\n",
    "    yhat = np.zeros((n_sequence, ny))\n",
    "    hprev = None\n",
    "\n",
    "    for t, x in enumerate(xs):\n",
    "        if t == 0:\n",
    "            hprev = np.zeros((1, nh))\n",
    "        else:\n",
    "            hprev = h[t - 1]\n",
    "\n",
    "        a[t] = np.matmul(x, Wxh) + np.matmul(hprev, Whh) + bh\n",
    "        h[t] = np.tanh(a[t])\n",
    "        o[t] = np.matmul(h[t], Why) + by\n",
    "        yhat[t] = softmax(o[t])\n",
    "    return yhat, a, h, o\n",
    "\n",
    "def forward(batch_xs, weights):\n",
    "    batch_yhat = []\n",
    "    batch_a = []\n",
    "    batch_o = []\n",
    "    batch_h = []\n",
    "\n",
    "    for xs in batch_xs:\n",
    "        yhat, a, h, o = _forward(xs, weights)\n",
    "        batch_yhat.append(yhat)\n",
    "        batch_o.append(o)\n",
    "        batch_h.append(h)\n",
    "        batch_a.append(a)\n",
    "\n",
    "    batch_yhat = np.array(batch_yhat)\n",
    "    batch_a = np.array(batch_a)\n",
    "    batch_h = np.array(batch_h)\n",
    "    batch_o = np.array(batch_o)\n",
    "    return batch_yhat, batch_a, batch_h, batch_o\n",
    "\n",
    "def xentropy(y, yhat):\n",
    "    return np.mean(np.sum(-y * np.log(yhat + 1e-8), axis=0))\n",
    "\n",
    "\n",
    "def _backward(xs, ys, weights, a, o, h, yhat):\n",
    "    n_sequences = xs.shape[0]\n",
    "\n",
    "    Why = weights['Why']\n",
    "    Whh = weights['Whh']\n",
    "    Wxh = weights['Wxh']\n",
    "    bh = weights['bh']\n",
    "    by = weights['by']\n",
    "\n",
    "    grads = {name: np.zeros_like(weights[name]) for name in weights}\n",
    "    danext = None\n",
    "    for i in range(n_sequences - 1, -1, -1):\n",
    "        if i == n_sequences - 1:\n",
    "            danext = np.zeros_like(a[i:i + 1])\n",
    "\n",
    "        dot = yhat[i:i + 1] - ys[i:i + 1]\n",
    "\n",
    "        # backprop through ot\n",
    "        dby = dot\n",
    "        dWhy = np.matmul(h[i:i + 1].T, dot)\n",
    "        dht = np.matmul(dot, Why.T) + np.matmul(danext, Whh.T)\n",
    "        dWhh = np.matmul(h[i:i + 1].T, danext)\n",
    "\n",
    "        # backprop through ht\n",
    "        dat = dht * (1 - h[i:i + 1] ** 2)\n",
    "\n",
    "        # backprop through at\n",
    "        dWxh = np.matmul(xs[i:i + 1].T, dat)\n",
    "        dbh = dat\n",
    "\n",
    "        # 累加梯度\n",
    "        grads['by'] += dby\n",
    "        grads['bh'] += dbh\n",
    "        grads['Whh'] += dWhh\n",
    "        grads['Wxh'] += dWxh\n",
    "        grads['Why'] += dWhy\n",
    "        danext = dat\n",
    "\n",
    "    for k in grads:\n",
    "        grads[k] = grads[k] / n_sequences\n",
    "    return grads\n",
    "\n",
    "def backward(batch_xs, batch_ys, weights, batch_a, batch_o, batch_h, batch_yhat):\n",
    "    n_batch = batch_xs.shape[0]\n",
    "    grads = {name: np.zeros_like(weights[name]) for name in weights}\n",
    "    for xs, ys, a, o, h, yhat in zip(batch_xs, batch_ys, batch_a, batch_o, batch_h, batch_yhat):\n",
    "        tmp_grads = _backward(xs, ys, weights, a, o, h, yhat)\n",
    "        for k in tmp_grads:\n",
    "            grads[k] += tmp_grads[k]\n",
    "    for k in grads:\n",
    "        grads[k] /= n_batch\n",
    "    return grads\n",
    "\n",
    "\n",
    "yhat, a, h, o = forward(x_train, weights)\n",
    "loss = xentropy(y_train, yhat)\n",
    "grads = backward(x_train, y_train, weights, a, o, h, yhat)\n",
    "for name in grads:\n",
    "    print(name)\n",
    "    print(grads[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 中的 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证我们计算的结果，我们使用 Tensorflow 中的 RNN 来验证我们的结果，笔者使用的 tensorflow 的版本是 2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.145563Z",
     "start_time": "2020-07-10T03:14:11.706125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:20:31.978187Z",
     "start_time": "2020-07-02T10:20:31.965261Z"
    }
   },
   "source": [
    "Tensorflow 中的权重会默认初始化的权重和我们自定义的权重不一样，因此我们自定义一个权重，供 tensorflow 来初始化权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.157837Z",
     "start_time": "2020-07-10T03:14:13.146945Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "def tf_get_weights(shape, dtype=None):\n",
    "    np.random.seed(123)\n",
    "    return tf.Variable(np.random.randn(*shape), dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Tensorflow 中，使用 tf.kears.layers.SimpleRNN 来定义 RNN 层。\n",
    "\n",
    "第一个参数表示 RNN 层的神经元个数，相当于我们的 nh。\n",
    "return_sequences=True表示输出的是一个 vector 序列。在我们的应用中，输出的是3个vector，是一个 vector 序列，因此这个参数赋值为 True\n",
    "kernel_initializer 对应的是 Wxh 的初始化函数。这里我们用自定义的 `tf_get_weights`\n",
    "recurrent_initializer 对应的是 Whh 的初始化函数，这里我们也用自定义的 `tf_get_weights`\n",
    "至于 by 和 bh，tensorflow 默认初始化为0，和我们的初始化相同，因此就不需要设置了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.221990Z",
     "start_time": "2020-07-10T03:14:13.159015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer simple_rnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    SimpleRNN(nh, return_sequences=True, kernel_initializer=tf_get_weights, recurrent_initializer=tf_get_weights), \n",
    "    Dense(n_features, kernel_initializer=tf_get_weights, activation='softmax')\n",
    "])\n",
    "\n",
    "xentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "with tf.GradientTape() as tape:\n",
    "    yhat  = model(x_train)\n",
    "    loss = xentropy(y_train, yhat)\n",
    "tf_grads_ = tape.gradient(loss, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出的梯度和 model.trainable_weights 中的变量一一对应，但是没有名字。我们将输出的梯度和 model.trainable_weights 的名字一一对应起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.225962Z",
     "start_time": "2020-07-10T03:14:13.223465Z"
    }
   },
   "outputs": [],
   "source": [
    "names = [variable.name for variable in model.trainable_weights]\n",
    "tf_grads = dict(zip(names, tf_grads_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:09:23.578570Z",
     "start_time": "2020-07-10T03:09:23.575699Z"
    }
   },
   "source": [
    "我们查看我们自己计算出的结果，并和 tensorflow 计算出的结果比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.231049Z",
     "start_time": "2020-07-10T03:14:13.227377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.23370055e-03  1.04331255e-01  2.23508645e-02  1.82005409e-02]\n",
      " [-1.54104326e-02 -3.32658598e-03 -9.04711414e-05 -1.50557747e-02]\n",
      " [ 8.19845311e-03  3.85644659e-02 -3.31628546e-02 -4.46483115e-04]\n",
      " [ 1.11874761e-02  1.59848616e-01 -2.25059371e-02  4.80860807e-02]\n",
      " [ 4.41634879e-02  7.01074908e-03  4.23029438e-02  9.37758684e-02]\n",
      " [ 8.51382967e-03 -4.64937501e-02 -1.44467270e-02  1.96995456e-02]\n",
      " [-2.08229781e-03 -1.86435338e-02 -1.15537411e-02  5.49922101e-02]\n",
      " [-1.82566326e-02  1.89795326e-02 -3.24461833e-02 -1.32172499e-02]\n",
      " [-1.33043816e-02 -1.09042577e-01  2.38811933e-02  1.03872553e-01]\n",
      " [ 8.72361809e-02 -3.63293923e-02 -1.23410830e-02 -1.97669547e-02]\n",
      " [ 1.14516485e-02 -4.60840343e-03  3.22320871e-02  2.18383409e-02]\n",
      " [ 6.77897185e-02  1.38952746e-03 -5.20812087e-02 -5.00065386e-02]]\n",
      "tf.Tensor(\n",
      "[[-6.23372197e-03  1.04331210e-01  2.23508514e-02  1.82005260e-02]\n",
      " [-1.54104307e-02 -3.32658668e-03 -9.04710541e-05 -1.50557710e-02]\n",
      " [ 8.19845218e-03  3.85644622e-02 -3.31628509e-02 -4.46482591e-04]\n",
      " [ 1.11874724e-02  1.59848616e-01 -2.25059278e-02  4.80860770e-02]\n",
      " [ 4.41635028e-02  7.01076444e-03  4.23029400e-02  9.37758982e-02]\n",
      " [ 8.51384085e-03 -4.64937463e-02 -1.44467223e-02  1.96995549e-02]\n",
      " [-2.08230410e-03 -1.86435431e-02 -1.15537141e-02  5.49922213e-02]\n",
      " [-1.82566326e-02  1.89795345e-02 -3.24461758e-02 -1.32172396e-02]\n",
      " [-1.33044170e-02 -1.09042577e-01  2.38811877e-02  1.03872545e-01]\n",
      " [ 8.72362107e-02 -3.63293551e-02 -1.23410895e-02 -1.97669752e-02]\n",
      " [ 1.14516355e-02 -4.60841320e-03  3.22320834e-02  2.18383372e-02]\n",
      " [ 6.77897334e-02  1.38957892e-03 -5.20812273e-02 -5.00065424e-02]], shape=(12, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(grads['Wxh'])\n",
    "print(tf_grads['sequential/simple_rnn/kernel:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.235267Z",
     "start_time": "2020-07-10T03:14:13.232209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15446615 -0.00124231 -0.05664341 -0.16452336]\n",
      " [ 0.13315888  0.08278523 -0.04694551 -0.12312789]\n",
      " [ 0.0983765   0.00854117  0.00313957  0.06394426]\n",
      " [ 0.04713476 -0.08556058 -0.04893308 -0.06553293]]\n",
      "tf.Tensor(\n",
      "[[ 0.15446615 -0.00124224 -0.05664344 -0.16452336]\n",
      " [ 0.13315882  0.08278525 -0.04694556 -0.1231279 ]\n",
      " [ 0.09837651  0.00854114  0.00313959  0.06394422]\n",
      " [ 0.04713476 -0.08556058 -0.04893309 -0.06553293]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(grads['Whh'])\n",
    "print(tf_grads['sequential/simple_rnn/recurrent_kernel:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.239938Z",
     "start_time": "2020-07-10T03:14:13.237011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18325336  0.11167993 -0.05786112  0.26197213]]\n",
      "tf.Tensor([ 0.18325335  0.11167993 -0.0578611   0.26197213], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(grads['bh'])\n",
    "print(tf_grads['sequential/simple_rnn/bias:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.244457Z",
     "start_time": "2020-07-10T03:14:13.241193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09955249  0.05676479  0.04074105 -0.0183601  -0.03368903  0.07818042\n",
      "   0.04349705  0.01167354  0.00267598 -0.02386261 -0.01919586 -0.03887272]]\n",
      "tf.Tensor(\n",
      "[-0.0995525   0.05676478  0.04074106 -0.0183601  -0.03368903  0.07818042\n",
      "  0.04349704  0.01167355  0.00267598 -0.02386262 -0.01919586 -0.03887273], shape=(12,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(grads['by'])\n",
    "print(tf_grads['sequential/dense/bias:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T03:14:13.250423Z",
     "start_time": "2020-07-10T03:14:13.245847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01225391  0.0329933  -0.02681418 -0.00649567 -0.05570216  0.1297125\n",
      "  -0.03257607  0.01844829  0.10973296 -0.0778837  -0.01666309 -0.06249831]\n",
      " [ 0.0232008  -0.03062716 -0.07174666  0.00251815  0.00192849  0.10913263\n",
      "   0.11045782  0.05964187 -0.03315546 -0.04200283 -0.0741348  -0.05521285]\n",
      " [-0.01224116  0.00566421  0.11142957 -0.06263226  0.04732599 -0.00419463\n",
      "  -0.03436515 -0.00857714 -0.01389913 -0.00123701 -0.00276442 -0.02450886]\n",
      " [ 0.05996267  0.01078336 -0.05233568  0.03773454 -0.07731857 -0.10633167\n",
      "  -0.09503508  0.00923052  0.10124347 -0.0650825   0.14232817  0.03482077]]\n",
      "tf.Tensor(\n",
      "[[-0.01225391  0.0329933  -0.02681417 -0.00649568 -0.05570215  0.12971252\n",
      "  -0.03257608  0.01844828  0.10973296 -0.07788371 -0.01666309 -0.06249831]\n",
      " [ 0.0232008  -0.03062716 -0.07174667  0.00251815  0.0019285   0.10913265\n",
      "   0.11045782  0.05964188 -0.03315546 -0.04200282 -0.07413479 -0.05521284]\n",
      " [-0.01224115  0.00566421  0.11142958 -0.06263226  0.04732599 -0.00419462\n",
      "  -0.03436515 -0.00857716 -0.01389912 -0.00123701 -0.00276444 -0.02450886]\n",
      " [ 0.05996269  0.01078336 -0.05233568  0.03773453 -0.07731859 -0.10633168\n",
      "  -0.09503508  0.00923053  0.10124347 -0.06508251  0.14232814  0.03482078]], shape=(4, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(grads['Why'])\n",
    "print(tf_grads['sequential/dense/kernel:0'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
