{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# RNN Numpy 实现二—— 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们取输入序列为 `abcdefghijklmnopqrstuvwxyz`，用这个序列来产生我们的训练样本。我们使用 one-hot encoding 来将字母编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "input_word = list(\"I am learning RNN\")\n",
    "words = set(input_word)\n",
    "input_word_onehot = OneHotEncoder(sparse=False).fit_transform(np.array(input_word).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来完成一个 Character-Level Language Models 的任务：用前一个字母来预测后一个字母。\n",
    "\n",
    "假设我们有一句话，\"hello\"。我们希望：\n",
    "输入 h 输出 e，\n",
    "输入 he，输出 l, \n",
    "输入 hel，输出 l 。\n",
    "\n",
    "这里，我们取序列长度为 3，因此我们的一个样本应该是\n",
    "\n",
    "((h, e, l), (e, l, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列长度取 3\n",
    "sequenceLen = 3\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(input_word_onehot) - sequenceLen):\n",
    "    x.append(input_word_onehot[i:i + sequenceLen])\n",
    "    y.append(input_word_onehot[i + 1: i + 1 + sequenceLen])  \n",
    "\n",
    "x_train = np.array(x)\n",
    "y_train = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义两个初始化weight 和 bias 的函数 `get_weights` 和 `get_bias`，输入形状和类型就可以初始化 weight 和 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(shape, dtype=np.float32):\n",
    "    np.random.seed(123)\n",
    "    return np.array(np.random.randn(*shape), dtype=dtype)\n",
    "\n",
    "def get_bias(shape, dtype=np.float32):\n",
    "    return np.zeros(shape, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一些形状相关的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = len(words) # 所有字母的个数\n",
    "nx = n_class\n",
    "ny = n_class\n",
    "nh = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便管理，将 weight 和 bias 放在一个字典中，并初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'Wxh': get_weights((nx, nh)),\n",
    "    'Why': get_weights((nh, ny)),\n",
    "    'Whh': get_weights((nh, nh)),\n",
    "    'bh': get_bias((1, nh)),\n",
    "    'by': get_bias((1, ny))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个 softmax 函数，之后会用到，注意防止溢出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简化思考，我们先考虑只输入一个样本的情况。\n",
    "\n",
    "每个样本的形状为 (3 x 12, 3 x 12)，X 包含三个词，每个词 one-hot 之后的形状为 12，因此是 3 x 12。 Y 也包含三个词，因此形状也是 3 x 12。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(xs, weights):\n",
    "    Why = weights['Why']\n",
    "    Whh = weights['Whh']\n",
    "    Wxh = weights['Wxh']\n",
    "    bh = weights['bh']\n",
    "    by = weights['by']\n",
    "\n",
    "    n_sequence = xs.shape[0]\n",
    "    ny = Why.shape[1]\n",
    "    nh = Wxh.shape[1]\n",
    "\n",
    "    a = np.zeros((n_sequence, nh))\n",
    "    h = np.zeros((n_sequence, nh))\n",
    "    o = np.zeros((n_sequence, ny))\n",
    "    yhat = np.zeros((n_sequence, ny))\n",
    "    hprev = None\n",
    "\n",
    "    for t, x in enumerate(xs):\n",
    "        if t == 0:\n",
    "            hprev = np.zeros((1, nh))\n",
    "        else:\n",
    "            hprev = h[t - 1]\n",
    "\n",
    "        a[t] = np.matmul(x, Wxh) + np.matmul(hprev, Whh) + bh\n",
    "        h[t] = np.tanh(a[t])\n",
    "        o[t] = np.matmul(h[t], Why) + by\n",
    "        yhat[t] = softmax(o[t])\n",
    "    return yhat, a, h, o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播直接应用上一篇计算出来的公式即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backward(xs, ys, weights, a, o, h, yhat):\n",
    "    n_sequences = xs.shape[0]\n",
    "\n",
    "    Why = weights['Why']\n",
    "    Whh = weights['Whh']\n",
    "    Wxh = weights['Wxh']\n",
    "    bh = weights['bh']\n",
    "    by = weights['by']\n",
    "\n",
    "    grads = {name: np.zeros_like(weights[name]) for name in weights}\n",
    "    danext = None\n",
    "    for i in range(n_sequences - 1, -1, -1):\n",
    "        if i == n_sequences - 1:\n",
    "            danext = np.zeros_like(a[i:i + 1])\n",
    "\n",
    "        dot = yhat[i:i + 1] - ys[i:i + 1]\n",
    "\n",
    "        # backprop through ot\n",
    "        dby = dot\n",
    "        dWhy = np.matmul(h[i:i + 1].T, dot)\n",
    "        dht = np.matmul(dot, Why.T) + np.matmul(danext, Whh.T)\n",
    "        dWhh = np.matmul(h[i:i + 1].T, danext)\n",
    "\n",
    "        # backprop through ht\n",
    "        dat = dht * (1 - h[i:i + 1] ** 2)\n",
    "\n",
    "        # backprop through at\n",
    "        dWxh = np.matmul(xs[i:i + 1].T, dat)\n",
    "        dbh = dat\n",
    "\n",
    "        # 累加梯度\n",
    "        grads['by'] += dby\n",
    "        grads['bh'] += dbh\n",
    "        grads['Whh'] += dWhh\n",
    "        grads['Wxh'] += dWxh\n",
    "        grads['Why'] += dWhy\n",
    "        danext = dat\n",
    "\n",
    "    for k in grads:\n",
    "        grads[k] = grads[k] / n_sequences\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算结果，\b这里我们只输入一个样本测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wxh\n[[-5.8980067e-03 -1.0567507e-03  5.3060856e-02  3.2835844e-01]\n [-2.1574606e-01 -4.6572205e-02 -1.2665960e-03 -2.1078084e-01]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n [ 2.6740572e-01 -2.2931943e-04 -1.3003336e-01  3.1455082e-03]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]]\nWhy\n[[ 8.50889087e-02  6.36707619e-03 -7.47825659e-04 -1.98840220e-02\n   2.58626133e-01 -2.30374616e-02 -1.09052859e-01 -2.62917131e-02\n   4.05798629e-02 -2.19466284e-01  3.74572002e-03  4.07243753e-03]\n [-1.46826804e-01  1.54953711e-02  2.54969578e-03  3.39287072e-02\n  -2.12813720e-01  9.99306440e-02  1.29649192e-01  1.57209918e-01\n   1.14360772e-01 -2.42030308e-01  7.84051884e-03  4.07059975e-02]\n [ 1.81494430e-01 -1.09105455e-02 -2.18753633e-03 -3.09268292e-02\n   1.77046895e-01 -8.63371193e-02 -1.17713965e-01 -1.33606702e-01\n  -8.24294090e-02  1.42429918e-01 -5.35984524e-03 -3.14992703e-02]\n [ 8.77180919e-02  1.24198990e-02 -2.34871477e-05 -1.14589296e-02\n   1.88178286e-01  4.68511926e-03 -7.55594000e-02  1.88627150e-02\n   8.38674083e-02 -3.33614200e-01  6.92562899e-03  1.79988574e-02]]\nWhh\n[[-0.262735    0.0007792   0.10157917 -0.17441405]\n [ 0.2608917  -0.00121021 -0.08023392  0.30819622]\n [-0.22259253  0.00123626  0.0588261  -0.325961  ]\n [-0.182428    0.00058585  0.06841204 -0.13496612]]\nbh\n[[ 0.04576164 -0.04785828 -0.07823911  0.1207231 ]]\nby\n[[-0.16537945  0.01559837  0.00261957  0.0350502  -0.21054842  0.1030496\n   0.13270438  0.16181064  0.1154549  -0.23959424  0.00786391  0.04137054]]\n"
    }
   ],
   "source": [
    "x, y = x_train[0], y_train[0]\n",
    "yhat, a, h, o = forward(x, weights)\n",
    "grads = backward(x, y, weights, a, o, h, yhat)\n",
    "for name in grads:\n",
    "    print(name)\n",
    "    print(grads[name])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83",
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}