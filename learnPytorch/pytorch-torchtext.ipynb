{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# Pytorch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:15:55.811962Z",
     "start_time": "2020-10-21T07:15:54.302415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.doubanio.com/simple/\r\n",
      "Requirement already satisfied: torchtext in /anaconda3/lib/python3.7/site-packages (0.7.0)\r\n",
      "Requirement already satisfied: sentencepiece in /anaconda3/lib/python3.7/site-packages (from torchtext) (0.1.92)\r\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.7/site-packages (from torchtext) (4.31.1)\r\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from torchtext) (1.19.1)\r\n",
      "Requirement already satisfied: torch in /anaconda3/lib/python3.7/site-packages (from torchtext) (1.6.0)\r\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from torchtext) (2.9.1)\r\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from torch->torchtext) (0.17.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchtext概述\n",
    "\n",
    "torchtext预处理流程：\n",
    "\n",
    "1. 定义Field：声明如何处理数据\n",
    "2. 定义Dataset：得到数据集，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist\n",
    "3. 建立vocab：在这一步建立词汇表，词向量(word embeddings)\n",
    "4. 构造Iterator，用来分批次训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field对象\n",
    "\n",
    "Field对象指定要如何处理某个字段."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Dataset定义数据源信息."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator\n",
    "\n",
    "迭代器返回模型所需要的处理后的数据.迭代器主要分为Iterator, BucketIerator, BPTTIterator三种。\n",
    "\n",
    "*   Iterator：标准迭代器\n",
    "*   BucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。除此之外，我们还可以在Field中通过 fix_length参数来对样本进行截断补齐操作。\n",
    "*   BPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sequential – Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\n",
    "\n",
    "- use_vocab – Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.\n",
    "\n",
    "- init_token – A token that will be prepended to every example using this field, or None for no initial token. Default: None.\n",
    "\n",
    "- eos_token – A token that will be appended to every example using this field, or None for no end-of-sentence token. Default: None.\n",
    "\n",
    "- fix_length – A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. Default: None.\n",
    "\n",
    "- dtype – The torch.dtype class that represents a batch of examples of this kind of data. Default: torch.long.\n",
    "\n",
    "- preprocessing – The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.\n",
    "\n",
    "- postprocessing – A Pipeline that will be applied to examples using this field after numericalizing but before the numbers are turned into a Tensor. The pipeline function takes the batch as a list, and the field’s Vocab. Default: None.\n",
    "\n",
    "- lower – Whether to lowercase the text in this field. Default: False.\n",
    "\n",
    "- tokenize – The function used to tokenize strings using this field into sequential examples. If “spacy”, the SpaCy tokenizer is used. If a non-serializable function is passed as an argument, the field will not be able to be serialized. Default: string.split.\n",
    "\n",
    "- tokenizer_language – The language of the tokenizer to be constructed. Various languages currently supported only in SpaCy.\n",
    "\n",
    "- include_lengths – Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.\n",
    "\n",
    "- batch_first – Whether to produce tensors with the batch dimension first. Default: False.\n",
    "\n",
    "- pad_token – The string token used as padding. Default: “<pad>”.\n",
    "\n",
    "- unk_token – The string token used to represent OOV words. Default: “<unk>”.\n",
    "\n",
    "- pad_first – Do the padding of the sequence at the beginning. Default: False.\n",
    "\n",
    "- truncate_first – Do the truncating of the sequence at the beginning. Default: False\n",
    "\n",
    "- stop_words – Tokens to discard during the preprocessing step. Default: None\n",
    "\n",
    "- is_target – Whether this field is a target variable. Affects iteration over batches. Default: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:15:56.262418Z",
     "start_time": "2020-10-21T07:15:55.814040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(sequential=True, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 LABEL 来说，不同的任务的 LABEL 不同。对于机器翻译来说，LABEL 也是文本，因此也需要分词，此时和 TEXT 的定义类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:15:56.266924Z",
     "start_time": "2020-10-21T07:15:56.264276Z"
    }
   },
   "outputs": [],
   "source": [
    "LABEL = data.Field(sequential=True, lower=True, fix_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们还将文本的长度固定为 fix_length=20，这样我们在预测的时候就可以固定输出长度为 20 的句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对于一些其他任务，如情感分类，此时 label 可能就不是文本。如 label 是 0 或 1，分别表示 postive 和 negative，因此不需要 sequenital = False 和 use_vocab=False，此时 LABEL 的定义如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:15:56.271529Z",
     "start_time": "2020-10-21T07:15:56.268657Z"
    }
   },
   "outputs": [],
   "source": [
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对句子预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要预处理，可以修改 Field 的 preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:15:56.281185Z",
     "start_time": "2020-10-21T07:15:56.272848Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string\n",
    "\n",
    "TEXT = data.Field(sequential=True, lower=True, batch_first=True)\n",
    "TEXT.preprocessing = data.Pipeline(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用jieba分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以传入 tokenize 参数来自定义如何分词，demo 来自 [ 5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:23:29.222953Z",
     "start_time": "2020-10-21T07:23:29.218043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "\n",
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    定义TEXT的tokenize规则\n",
    "    \"\"\"\n",
    "\n",
    "    #去掉不在(所有中文、大小写字母、数字)中的非法字符\n",
    "    regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
    "    text = regex.sub(' ', text)\n",
    "\n",
    "    #使用jieba分词\n",
    "    return [word for word in jieba.cut(text) if word.strip()]\n",
    "\n",
    "TEXT = data.Field(lower=True, tokenize = tokenizer)\n",
    "LABEL = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:15:56.354259Z",
     "start_time": "2020-10-21T07:15:56.282829Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-55ef565aff2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m train,val = data.TabularDataset.splits(\n\u001b[1;32m      7\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         fields=[('PhraseId',None),('SentenceId',None),('Phrase', TEXT), ('Sentiment', LABEL)])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m test = data.TabularDataset('test.tsv', format='tsv',skip_header=True,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         train_data = None if train is None else cls(\n\u001b[0;32m---> 78\u001b[0;31m             os.path.join(path, train), **kwargs)\n\u001b[0m\u001b[1;32m     79\u001b[0m         val_data = None if validation is None else cls(\n\u001b[1;32m     80\u001b[0m             os.path.join(path, validation), **kwargs)\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m             'tsv': Example.fromCSV, 'csv': Example.fromCSV}[format]\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode_csv_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcsv_reader_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "我们不需要 'PhraseId' 和 'SentenceId'这两列, 所以我们给他们的field传递 None\n",
    "如果你的数据有列名，如我们这里的'Phrase','Sentiment',...\n",
    "设置skip_header=True,不然它会把列名也当一个数据处理\n",
    "\"\"\"\n",
    "train,val = data.TabularDataset.splits(\n",
    "        path='.', train='train.csv',validation='val.csv', format='csv',skip_header=True,\n",
    "        fields=[('PhraseId',None),('SentenceId',None),('Phrase', TEXT), ('Sentiment', LABEL)])\n",
    "\n",
    "test = data.TabularDataset('test.tsv', format='tsv',skip_header=True,\n",
    "        fields=[('PhraseId',None),('SentenceId',None),('Phrase', TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:16:19.755103Z",
     "start_time": "2020-10-21T07:16:19.746520Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-793d4faef684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "print(train[5])\n",
    "print(train[5].__dict__.keys())\n",
    "print(train[5].Phrase, train[0].Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到第6行的输入，它是一个Example对象。Example对象绑定了一行中的所有属性，可以看到，句子已经被分词了，但是没有转化为数字。\n",
    "\n",
    "这是因为我们还没有建立vocab，我们将在下一步建立vocab。\n",
    "\n",
    "Torchtext可以将词转化为数字，但是它需要被告知需要被处理的全部范围的词。我们可以用下面这行代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在建立词表的过程中，默认 `<unk>` 的 index 是 0，然后依次添加其他词语。因此会在有的代码中出现下面的对齐下标的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:16:23.166569Z",
     "start_time": "2020-10-21T07:16:23.158671Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8b25a5e8e625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# batch first, index align\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_iter' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in data_iter:\n",
    "    feature, target = batch.text, batch.label\n",
    "    feature.t_(), target.sub_(1)  # batch first, index align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义 Dataset 需要继承 `torchtext.data.Dataset` 类，其构造函数为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:16:41.074260Z",
     "start_time": "2020-10-21T07:16:41.066615Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchtext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-06f5f2703ca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torchtext' is not defined"
     ]
    }
   ],
   "source": [
    "torchtext.data.Dataset(examples, fields, filter_pred=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到它接受 examples，是一个 example 对象的列表，还有一个 fileds。下面的示例给出了一个自定义 Dataset 的例子."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:17:07.570860Z",
     "start_time": "2020-10-21T07:17:07.387800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data\n",
    "import os\n",
    "import random\n",
    "\n",
    "TEXT = data.Field(sequential=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "class MR(data.Dataset):\n",
    "    @classmethod\n",
    "    def splits(cls, path, fields=None, test_rate=0.1, shuffle=True, **kwargs):\n",
    "        examples = []\n",
    "        with open(os.path.join(path, \"rt-polarity.neg\")) as f:\n",
    "            examples += [data.Example.fromlist([line.strip(), 0], fields) for line in f]\n",
    "\n",
    "        with open(os.path.join(path, \"rt-polarity.pos\")) as f:\n",
    "            examples += [data.Example.fromlist([line.strip(), 1], fields) for line in f]\n",
    "\n",
    "        test_size = int(len(examples) * test_rate)\n",
    "        if shuffle: random.shuffle(examples)\n",
    "        test_examples = examples[:test_size]\n",
    "        train_examples = examples[test_size:]\n",
    "        return cls(train_examples, fields, **kwargs), cls(test_examples, fields, **kwargs)\n",
    "\n",
    "trainset, valset = MR.splits(\".\", fields=[(\"text\", TEXT), (\"label\", LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看到，我们分别从 `rt-polarity.neg` 和 `rt-polarity.pos` 中读取内容，并构造包含 data.Example 对象的 list。在最后调用构造函数返回 Dataset 对象."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:18:06.655679Z",
     "start_time": "2020-10-21T07:18:06.651527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__func__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__self__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建词表需要 Field 对象和 dataset 对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:17:21.172747Z",
     "start_time": "2020-10-21T07:17:21.088158Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab 建立之后就可以将词和数字之间相互转化了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:17:22.496782Z",
     "start_time": "2020-10-21T07:17:22.493146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depiction\n",
      "1387\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[1510])\n",
    "print(TEXT.vocab.stoi['bore'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们拿到一个没有分词的句子时，可以通过下列方式将其转化为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:17:26.345363Z",
     "start_time": "2020-10-21T07:17:26.341677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 90, 24, 2]\n"
     ]
    }
   ],
   "source": [
    "text = \"I love you .\"\n",
    "text = TEXT.preprocess(text) # ['i', 'love', 'you', '.']\n",
    "text_vec = [TEXT.vocab.stoi[word] for word in text] # [64, 83, 24, 2]\n",
    "print(text_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:15:56.366569Z",
     "start_time": "2020-10-21T07:15:54.561Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_iter = data.BucketIterator(trainset, batch_size=128, sort_key=lambda x: len(x.Phrase), \n",
    "                                 shuffle=True,device=device)\n",
    "\n",
    "val_iter = data.BucketIterator(valset, batch_size=128, sort_key=lambda x: len(x.Phrase), \n",
    "                                 shuffle=True,device=device)\n",
    "\n",
    "# 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序\n",
    "test_iter = data.Iterator(dataset=test, batch_size=128, train=False,\n",
    "                          sort=False, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的属性名就是我们之前在 fields 中设置的属性名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T07:15:56.367746Z",
     "start_time": "2020-10-21T07:15:54.573Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in train_iter:\n",
    "    data = batch.Phrase\n",
    "    label = batch.Sentiment\n",
    "    print(batch.Phrase.shape)\n",
    "    print(batch.Phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API\n",
    "\n",
    "#### vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numericalize\n",
    "\n",
    "Field.numericalize([['eward', 'elric']]) 将词语转化为 one-hot 表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. [TorchText用法示例及完整代码_nlpuser的博客-CSDN博客](https://blog.csdn.net/nlpuser/article/details/88067167)\n",
    "2. [Torchtext使用教程 文本数据处理 - 林震宇 - 博客园](https://www.cnblogs.com/linzhenyu/p/13277552.html)\n",
    "3. [Torchtext使用教程_NLP Tutorial-CSDN博客](https://blog.csdn.net/JWoswin/article/details/92821752)\n",
    "4. [torchtext.data — torchtext 0.8.0a0+c4a91f2 documentation](https://pytorch.org/text/data.html#dataset-batch-and-example)\n",
    "\n",
    "5. [torchtext极简实用的pipeline - 知乎](https://zhuanlan.zhihu.com/p/78341221)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
