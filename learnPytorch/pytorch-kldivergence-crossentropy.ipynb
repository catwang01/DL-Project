{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# Pytorch KL散度和交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在关于知识蒸馏的代码中，常常会见到使用 KLDiv 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T12:47:03.840655Z",
     "start_time": "2020-09-01T12:47:03.838152Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_fn_kd(outputs, labels, teacher_outputs, params):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities! See Issue #2\n",
    "    \"\"\"\n",
    "    alpha = params.alpha\n",
    "    T = params.temperature\n",
    "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) +\\\n",
    "                             F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    " \n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这其中有两个奇怪的地方：\n",
    "1. 为什么要使用 nn.KLDivLoss，使用 nn.CrossEntropy 不可以吗？\n",
    "2. 为什么要对 nn.KLDivLoss 的第一个参数取 log，而不对第二个参数取 log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T12:49:27.400225Z",
     "start_time": "2020-09-01T12:49:27.397839Z"
    }
   },
   "source": [
    "## 问题一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从道理上讲，最小化 crossentropy 和 最小化 kl divergence 是等价的，因为有下列等式成立："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T12:59:34.600866Z",
     "start_time": "2020-09-01T12:59:34.597190Z"
    }
   },
   "source": [
    "Entropy(p) + KL Divergence(p||q) = CrossEntropy(p||q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:01:52.509023Z",
     "start_time": "2020-09-01T13:01:52.505061Z"
    }
   },
   "source": [
    "其中，\n",
    "\n",
    "Entropy(p) = - p(x) log p(x)\n",
    "\n",
    "KL Divergence(p||q) = - p(x) log q(x) / p(x) = p(x) log p(x) - p(x) log q(x)\n",
    "\n",
    "CrossEntropy(p || q) = - p(x) log q(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 Entropy 对于一个固定的 p(x) 来说是固定，因此减小 KL Divergence 和 减小 CrossEntropy 是等价的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那为啥在网上的实现中都是最小化 KL Divergence，而不是最小化 CrossEntropy 呢？这个实际上是软件实现上的原因。\n",
    "\n",
    "因为 Pytorch 中的 nn.CrossEntropy 只支持 p 是 hard label，而 q 是 soft label 的情况。即 nn.CrossEntropy 不支持两个都是 soft label 的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而 nn.KLDivLoss 支持两个都是 soft_label 的情况。因此大家倾向于使用 nn.KLDivLoss。当然，理论上说，如果可以手动实现一个支持 p 和 q 都是 soft label 的 CrossEntropy 函数也是可以的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题二"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个问题和 `nn.KLDivLoss` 的功能有关。从文档中可以看到，`nn.KLDivLoss` 计算的是\n",
    "\n",
    "$$\n",
    "1(\\mathrm{x}, \\mathrm{y})=\\mathrm{L}=\\left\\{l_{1}, \\ldots, \\mathrm{l}_{\\mathrm{N}}\\right\\}, \\quad l_{\\mathrm{n}}=\\mathrm{y}_{\\mathrm{n}} \\cdot\\left(\\log \\mathrm{y}_{\\mathrm{n}}-\\mathrm{x}_{\\mathrm{n}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的 $y$ 对应上述公式中的 $p(x)$，而 $x$ 对应上述公式中的 $q(x)$。可以看到，`nn.KLDivLoss` 计算时，并不会对 x 取 log。因此为了正确计算 KL Divergence，需要手动添加 log。这也就是为什么第一个参数会添加 log 的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [(4条消息)知识蒸馏（Knowledge Distillation）_AI Flash-CSDN博客](https://blog.csdn.net/nature553863/article/details/80568658)\n",
    "2. [KLDivLoss — PyTorch 1.6.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=nn%20kldiv#torch.nn.KLDivLoss)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
